{
 "metadata": {
  "name": "",
  "signature": "sha256:fa46d608e17dc596ab981ab13f9df56cd36f2f4536e83cd54609e1aa0efd99c2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Logistic and Other Classifiers -- Categorizing Flowers\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook provides:\n",
      "* A brief background on logistic classification\n",
      "* A mesh function using [np.meshgrid](http://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html) to evaluate the predictive functions on a 2 dimensional feature grid\n",
      "\n",
      "The intention is to provide a starting template with which to contrast various classifiers on a clean, real-world data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "#import pylab as pl #May be necessary to use \"%pylab inline\" if graphs do not show in line.\n",
      "pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-121-0461000372e3>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-121-0461000372e3>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    pylab inline\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model, datasets\n",
      "from sklearn.cross_validation import train_test_split"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Acquire the data: Flower Power"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the data\n",
      "# take a look at the datasets available:\n",
      "#datasets."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import built in iris data set\n",
      "iris = datasets.load_iris()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# About the data\n",
      "#Take a look\n",
      "df = pd.DataFrame(iris.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(iris.feature_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.columns=(iris.feature_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['Flower Type']=pd.Series(iris.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(iris.DESCR)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iris Plants Database\n",
        "\n",
        "Notes\n",
        "-----\n",
        "Data Set Characteristics:\n",
        "    :Number of Instances: 150 (50 in each of three classes)\n",
        "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
        "    :Attribute Information:\n",
        "        - sepal length in cm\n",
        "        - sepal width in cm\n",
        "        - petal length in cm\n",
        "        - petal width in cm\n",
        "        - class:\n",
        "                - Iris-Setosa\n",
        "                - Iris-Versicolour\n",
        "                - Iris-Virginica\n",
        "    :Summary Statistics:\n",
        "    ============== ==== ==== ======= ===== ====================\n",
        "                    Min  Max   Mean    SD   Class Correlation\n",
        "    ============== ==== ==== ======= ===== ====================\n",
        "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
        "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
        "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
        "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
        "    ============== ==== ==== ======= ===== ====================\n",
        "    :Missing Attribute Values: None\n",
        "    :Class Distribution: 33.3% for each of 3 classes.\n",
        "    :Creator: R.A. Fisher\n",
        "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
        "    :Date: July, 1988\n",
        "\n",
        "This is a copy of UCI ML iris datasets.\n",
        "http://archive.ics.uci.edu/ml/datasets/Iris\n",
        "\n",
        "The famous Iris database, first used by Sir R.A Fisher\n",
        "\n",
        "This is perhaps the best known database to be found in the\n",
        "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
        "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
        "data set contains 3 classes of 50 instances each, where each class refers to a\n",
        "type of iris plant.  One class is linearly separable from the other 2; the\n",
        "latter are NOT linearly separable from each other.\n",
        "\n",
        "References\n",
        "----------\n",
        "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
        "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
        "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
        "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
        "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
        "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
        "     Structure and Classification Rule for Recognition in Partially Exposed\n",
        "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
        "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
        "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
        "     on Information Theory, May 1972, 431-433.\n",
        "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
        "     conceptual clustering system finds 3 classes in the data.\n",
        "   - Many, many more ...\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(iris.DESCR)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iris Plants Database\n",
        "\n",
        "Notes\n",
        "-----\n",
        "Data Set Characteristics:\n",
        "    :Number of Instances: 150 (50 in each of three classes)\n",
        "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
        "    :Attribute Information:\n",
        "        - sepal length in cm\n",
        "        - sepal width in cm\n",
        "        - petal length in cm\n",
        "        - petal width in cm\n",
        "        - class:\n",
        "                - Iris-Setosa\n",
        "                - Iris-Versicolour\n",
        "                - Iris-Virginica\n",
        "    :Summary Statistics:\n",
        "    ============== ==== ==== ======= ===== ====================\n",
        "                    Min  Max   Mean    SD   Class Correlation\n",
        "    ============== ==== ==== ======= ===== ====================\n",
        "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
        "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
        "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
        "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
        "    ============== ==== ==== ======= ===== ====================\n",
        "    :Missing Attribute Values: None\n",
        "    :Class Distribution: 33.3% for each of 3 classes.\n",
        "    :Creator: R.A. Fisher\n",
        "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
        "    :Date: July, 1988\n",
        "\n",
        "This is a copy of UCI ML iris datasets.\n",
        "http://archive.ics.uci.edu/ml/datasets/Iris\n",
        "\n",
        "The famous Iris database, first used by Sir R.A Fisher\n",
        "\n",
        "This is perhaps the best known database to be found in the\n",
        "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
        "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
        "data set contains 3 classes of 50 instances each, where each class refers to a\n",
        "type of iris plant.  One class is linearly separable from the other 2; the\n",
        "latter are NOT linearly separable from each other.\n",
        "\n",
        "References\n",
        "----------\n",
        "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
        "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
        "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
        "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
        "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
        "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
        "     Structure and Classification Rule for Recognition in Partially Exposed\n",
        "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
        "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
        "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
        "     on Information Theory, May 1972, 431-433.\n",
        "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
        "     conceptual clustering system finds 3 classes in the data.\n",
        "   - Many, many more ...\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Take a look\n",
      "df = pd.DataFrame(iris.data)\n",
      "df.columns=(iris.feature_names)\n",
      "df['Flower Type']=pd.Series(iris.target)\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>sepal length (cm)</th>\n",
        "      <th>sepal width (cm)</th>\n",
        "      <th>petal length (cm)</th>\n",
        "      <th>petal width (cm)</th>\n",
        "      <th>Flower Type</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 150.000000</td>\n",
        "      <td> 150.000000</td>\n",
        "      <td> 150.000000</td>\n",
        "      <td> 150.000000</td>\n",
        "      <td> 150.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>   5.843333</td>\n",
        "      <td>   3.054000</td>\n",
        "      <td>   3.758667</td>\n",
        "      <td>   1.198667</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>   0.828066</td>\n",
        "      <td>   0.433594</td>\n",
        "      <td>   1.764420</td>\n",
        "      <td>   0.763161</td>\n",
        "      <td>   0.819232</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>   4.300000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   0.100000</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>   5.100000</td>\n",
        "      <td>   2.800000</td>\n",
        "      <td>   1.600000</td>\n",
        "      <td>   0.300000</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>   5.800000</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>   4.350000</td>\n",
        "      <td>   1.300000</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>   6.400000</td>\n",
        "      <td>   3.300000</td>\n",
        "      <td>   5.100000</td>\n",
        "      <td>   1.800000</td>\n",
        "      <td>   2.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>   7.900000</td>\n",
        "      <td>   4.400000</td>\n",
        "      <td>   6.900000</td>\n",
        "      <td>   2.500000</td>\n",
        "      <td>   2.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 5 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
        "count         150.000000        150.000000         150.000000   \n",
        "mean            5.843333          3.054000           3.758667   \n",
        "std             0.828066          0.433594           1.764420   \n",
        "min             4.300000          2.000000           1.000000   \n",
        "25%             5.100000          2.800000           1.600000   \n",
        "50%             5.800000          3.000000           4.350000   \n",
        "75%             6.400000          3.300000           5.100000   \n",
        "max             7.900000          4.400000           6.900000   \n",
        "\n",
        "       petal width (cm)  Flower Type  \n",
        "count        150.000000   150.000000  \n",
        "mean           1.198667     1.000000  \n",
        "std            0.763161     0.819232  \n",
        "min            0.100000     0.000000  \n",
        "25%            0.300000     0.000000  \n",
        "50%            1.300000     1.000000  \n",
        "75%            1.800000     2.000000  \n",
        "max            2.500000     2.000000  \n",
        "\n",
        "[8 rows x 5 columns]"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean - Its already pretty clean thanks to Sir Ronald Fisher\n",
      "![](http://upload.wikimedia.org/wikipedia/commons/thumb/4/46/R._A._Fischer.jpg/200px-R._A._Fischer.jpg)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>sepal length (cm)</th>\n",
        "      <th>sepal width (cm)</th>\n",
        "      <th>petal length (cm)</th>\n",
        "      <th>petal width (cm)</th>\n",
        "      <th>Flower Type</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 5.1</td>\n",
        "      <td> 3.5</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 4.9</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 4.7</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 1.3</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4.6</td>\n",
        "      <td> 3.1</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 5.0</td>\n",
        "      <td> 3.6</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 5 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 132,
       "text": [
        "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
        "0                5.1               3.5                1.4               0.2   \n",
        "1                4.9               3.0                1.4               0.2   \n",
        "2                4.7               3.2                1.3               0.2   \n",
        "3                4.6               3.1                1.5               0.2   \n",
        "4                5.0               3.6                1.4               0.2   \n",
        "\n",
        "   Flower Type  \n",
        "0            0  \n",
        "1            0  \n",
        "2            0  \n",
        "3            0  \n",
        "4            0  \n",
        "\n",
        "[5 rows x 5 columns]"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Explore\n",
      "Our 3 Species:\n",
      "![](http://note.io/1iY4u5U)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.tools.plotting.scatter_matrix(df,alpha=.8, figsize=(15,15)) \n",
      "# Creates a scatter plot of all variables along with histograms for each variable in the diagonal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "array([[<matplotlib.axes.AxesSubplot object at 0x11251c090>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x111b6e450>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x1126d8a10>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x11299ec50>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x1129c7110>],\n",
        "       [<matplotlib.axes.AxesSubplot object at 0x1129a1f90>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x10f8efa90>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x10f916fd0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x10f936310>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x111b8aad0>],\n",
        "       [<matplotlib.axes.AxesSubplot object at 0x111b9e990>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x111bcee10>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x113203210>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x1132183d0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x11338aad0>],\n",
        "       [<matplotlib.axes.AxesSubplot object at 0x1133b0f50>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x11496fb50>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x1149965d0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x11497b110>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x1149d8ad0>],\n",
        "       [<matplotlib.axes.AxesSubplot object at 0x114e4ad90>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x114e69a90>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x114e911d0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x114ea40d0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x114ed55d0>]], dtype=object)"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Subset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We'll be using Sepal Width and Length, the first two columns, and will need numpy array to work with our SKLearn functions\n",
      "X = df.iloc[:,:2].values \n",
      "\n",
      "# This is equivalent to X = iris.data[:,:2] from the original data set.\n",
      "\n",
      "# And the target is the Flower Type column\n",
      "Y = df.iloc[:,4].values\n",
      "# Equivalent to Y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 135,
       "text": [
        "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 136,
       "text": [
        "array([[ 5.1,  3.5],\n",
        "       [ 4.9,  3. ],\n",
        "       [ 4.7,  3.2],\n",
        "       [ 4.6,  3.1],\n",
        "       [ 5. ,  3.6],\n",
        "       [ 5.4,  3.9],\n",
        "       [ 4.6,  3.4],\n",
        "       [ 5. ,  3.4],\n",
        "       [ 4.4,  2.9],\n",
        "       [ 4.9,  3.1],\n",
        "       [ 5.4,  3.7],\n",
        "       [ 4.8,  3.4],\n",
        "       [ 4.8,  3. ],\n",
        "       [ 4.3,  3. ],\n",
        "       [ 5.8,  4. ],\n",
        "       [ 5.7,  4.4],\n",
        "       [ 5.4,  3.9],\n",
        "       [ 5.1,  3.5],\n",
        "       [ 5.7,  3.8],\n",
        "       [ 5.1,  3.8],\n",
        "       [ 5.4,  3.4],\n",
        "       [ 5.1,  3.7],\n",
        "       [ 4.6,  3.6],\n",
        "       [ 5.1,  3.3],\n",
        "       [ 4.8,  3.4],\n",
        "       [ 5. ,  3. ],\n",
        "       [ 5. ,  3.4],\n",
        "       [ 5.2,  3.5],\n",
        "       [ 5.2,  3.4],\n",
        "       [ 4.7,  3.2],\n",
        "       [ 4.8,  3.1],\n",
        "       [ 5.4,  3.4],\n",
        "       [ 5.2,  4.1],\n",
        "       [ 5.5,  4.2],\n",
        "       [ 4.9,  3.1],\n",
        "       [ 5. ,  3.2],\n",
        "       [ 5.5,  3.5],\n",
        "       [ 4.9,  3.1],\n",
        "       [ 4.4,  3. ],\n",
        "       [ 5.1,  3.4],\n",
        "       [ 5. ,  3.5],\n",
        "       [ 4.5,  2.3],\n",
        "       [ 4.4,  3.2],\n",
        "       [ 5. ,  3.5],\n",
        "       [ 5.1,  3.8],\n",
        "       [ 4.8,  3. ],\n",
        "       [ 5.1,  3.8],\n",
        "       [ 4.6,  3.2],\n",
        "       [ 5.3,  3.7],\n",
        "       [ 5. ,  3.3],\n",
        "       [ 7. ,  3.2],\n",
        "       [ 6.4,  3.2],\n",
        "       [ 6.9,  3.1],\n",
        "       [ 5.5,  2.3],\n",
        "       [ 6.5,  2.8],\n",
        "       [ 5.7,  2.8],\n",
        "       [ 6.3,  3.3],\n",
        "       [ 4.9,  2.4],\n",
        "       [ 6.6,  2.9],\n",
        "       [ 5.2,  2.7],\n",
        "       [ 5. ,  2. ],\n",
        "       [ 5.9,  3. ],\n",
        "       [ 6. ,  2.2],\n",
        "       [ 6.1,  2.9],\n",
        "       [ 5.6,  2.9],\n",
        "       [ 6.7,  3.1],\n",
        "       [ 5.6,  3. ],\n",
        "       [ 5.8,  2.7],\n",
        "       [ 6.2,  2.2],\n",
        "       [ 5.6,  2.5],\n",
        "       [ 5.9,  3.2],\n",
        "       [ 6.1,  2.8],\n",
        "       [ 6.3,  2.5],\n",
        "       [ 6.1,  2.8],\n",
        "       [ 6.4,  2.9],\n",
        "       [ 6.6,  3. ],\n",
        "       [ 6.8,  2.8],\n",
        "       [ 6.7,  3. ],\n",
        "       [ 6. ,  2.9],\n",
        "       [ 5.7,  2.6],\n",
        "       [ 5.5,  2.4],\n",
        "       [ 5.5,  2.4],\n",
        "       [ 5.8,  2.7],\n",
        "       [ 6. ,  2.7],\n",
        "       [ 5.4,  3. ],\n",
        "       [ 6. ,  3.4],\n",
        "       [ 6.7,  3.1],\n",
        "       [ 6.3,  2.3],\n",
        "       [ 5.6,  3. ],\n",
        "       [ 5.5,  2.5],\n",
        "       [ 5.5,  2.6],\n",
        "       [ 6.1,  3. ],\n",
        "       [ 5.8,  2.6],\n",
        "       [ 5. ,  2.3],\n",
        "       [ 5.6,  2.7],\n",
        "       [ 5.7,  3. ],\n",
        "       [ 5.7,  2.9],\n",
        "       [ 6.2,  2.9],\n",
        "       [ 5.1,  2.5],\n",
        "       [ 5.7,  2.8],\n",
        "       [ 6.3,  3.3],\n",
        "       [ 5.8,  2.7],\n",
        "       [ 7.1,  3. ],\n",
        "       [ 6.3,  2.9],\n",
        "       [ 6.5,  3. ],\n",
        "       [ 7.6,  3. ],\n",
        "       [ 4.9,  2.5],\n",
        "       [ 7.3,  2.9],\n",
        "       [ 6.7,  2.5],\n",
        "       [ 7.2,  3.6],\n",
        "       [ 6.5,  3.2],\n",
        "       [ 6.4,  2.7],\n",
        "       [ 6.8,  3. ],\n",
        "       [ 5.7,  2.5],\n",
        "       [ 5.8,  2.8],\n",
        "       [ 6.4,  3.2],\n",
        "       [ 6.5,  3. ],\n",
        "       [ 7.7,  3.8],\n",
        "       [ 7.7,  2.6],\n",
        "       [ 6. ,  2.2],\n",
        "       [ 6.9,  3.2],\n",
        "       [ 5.6,  2.8],\n",
        "       [ 7.7,  2.8],\n",
        "       [ 6.3,  2.7],\n",
        "       [ 6.7,  3.3],\n",
        "       [ 7.2,  3.2],\n",
        "       [ 6.2,  2.8],\n",
        "       [ 6.1,  3. ],\n",
        "       [ 6.4,  2.8],\n",
        "       [ 7.2,  3. ],\n",
        "       [ 7.4,  2.8],\n",
        "       [ 7.9,  3.8],\n",
        "       [ 6.4,  2.8],\n",
        "       [ 6.3,  2.8],\n",
        "       [ 6.1,  2.6],\n",
        "       [ 7.7,  3. ],\n",
        "       [ 6.3,  3.4],\n",
        "       [ 6.4,  3.1],\n",
        "       [ 6. ,  3. ],\n",
        "       [ 6.9,  3.1],\n",
        "       [ 6.7,  3.1],\n",
        "       [ 6.9,  3.1],\n",
        "       [ 5.8,  2.7],\n",
        "       [ 6.8,  3.2],\n",
        "       [ 6.7,  3.3],\n",
        "       [ 6.7,  3. ],\n",
        "       [ 6.3,  2.5],\n",
        "       [ 6.5,  3. ],\n",
        "       [ 6.2,  3.4],\n",
        "       [ 5.9,  3. ]])"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Background on Logistic Classification\n",
      "\n",
      "* Binary Logistic Regression on Each Class vs. the Others\n",
      "\n",
      "* Logistic regression measures the relationship between a categorical dependent variable and one or more independent variables, which are usually (but not necessarily) continuous, by using probability scores as the predicted values of the dependent variable\n",
      "\n",
      "* constrain the predictions of the model to the range [0,1] so that we can interpret them as probability estimates. In Logistic Regression, we use the logit function to clamp predictions from the range [\u2212\u221e,\u221e] to [0,1]:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-10, 10, 100)\n",
      "y = 1.0 / (1.0 + np.exp(-x))\n",
      "import matplotlib.pyplot as plt\n",
      "plt.plot(x, y, 'r-', label='logit')\n",
      "plt.legend(loc='lower right')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 137,
       "text": [
        "<matplotlib.legend.Legend at 0x111bd8290>"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The logistic function takes on values between 0 and 1, with input B0 + B1X and output F(x) (like a linear regression)\n",
      "\n",
      "* Multiple logistic regression has B0 + B1X + B2X2, etc.\n",
      "\n",
      "* Regression coefficients can be evaluated\n",
      "\n",
      "The logistic function below takes a continuous linear equation for any value of X and binds it to an f(x) output which is between zero and one:\n",
      "![](http://upload.wikimedia.org/math/d/e/a/dea1753b35c226ec6999eb006c06f400.png)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Resources\n",
      "* [Great background](http://melodi.ee.washington.edu/~halloj3/classification.pdf) from John Halloran on logistic regression vs naive bayes with examples\n",
      "* Another excellent and clear [background](http://vassarstats.net/logreg1.html) with a working classifier to boot."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Now lets classify some Irisses!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Slice off a test set\n",
      "# Use train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25)\n",
      "\n",
      "# We can adjust the test_size to alter our test/train ratio"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "logreg_fitter = linear_model.LogisticRegression()\n",
      "# We create a new function of class LogisticRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We have our classifier!!!!\n",
      "# Fitted on the training data\n",
      "print X_train, Y_train\n",
      "logclf=logreg_fitter.fit(X_train, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 7.2  3. ]\n",
        " [ 5.6  2.5]\n",
        " [ 5.1  3.8]\n",
        " [ 5.2  2.7]\n",
        " [ 6.3  2.9]\n",
        " [ 7.7  2.8]\n",
        " [ 5.   2.3]\n",
        " [ 6.5  3. ]\n",
        " [ 4.6  3.1]\n",
        " [ 5.2  3.4]\n",
        " [ 5.5  2.3]\n",
        " [ 6.5  3. ]\n",
        " [ 6.3  3.3]\n",
        " [ 5.4  3.9]\n",
        " [ 6.3  2.5]\n",
        " [ 4.9  3.1]\n",
        " [ 6.   2.7]\n",
        " [ 5.1  2.5]\n",
        " [ 5.5  2.6]\n",
        " [ 5.   3.4]\n",
        " [ 7.1  3. ]\n",
        " [ 6.9  3.1]\n",
        " [ 5.5  2.5]\n",
        " [ 6.3  2.5]\n",
        " [ 5.4  3.4]\n",
        " [ 6.7  3. ]\n",
        " [ 6.7  3.1]\n",
        " [ 4.8  3.1]\n",
        " [ 5.8  2.7]\n",
        " [ 6.2  2.2]\n",
        " [ 5.7  4.4]\n",
        " [ 5.8  2.6]\n",
        " [ 6.7  3.1]\n",
        " [ 5.9  3.2]\n",
        " [ 6.8  3.2]\n",
        " [ 5.   3.4]\n",
        " [ 4.8  3. ]\n",
        " [ 6.8  3. ]\n",
        " [ 5.1  3.7]\n",
        " [ 5.6  3. ]\n",
        " [ 6.7  3.3]\n",
        " [ 5.   3.5]\n",
        " [ 4.5  2.3]\n",
        " [ 7.4  2.8]\n",
        " [ 5.7  2.8]\n",
        " [ 6.4  2.8]\n",
        " [ 6.3  3.3]\n",
        " [ 5.8  2.7]\n",
        " [ 7.   3.2]\n",
        " [ 6.4  2.7]\n",
        " [ 6.4  3.2]\n",
        " [ 6.   3.4]\n",
        " [ 6.6  2.9]\n",
        " [ 4.9  2.5]\n",
        " [ 5.8  2.7]\n",
        " [ 6.9  3.1]\n",
        " [ 5.5  4.2]\n",
        " [ 5.   3.3]\n",
        " [ 5.7  2.8]\n",
        " [ 6.1  2.9]\n",
        " [ 6.2  2.9]\n",
        " [ 4.9  3. ]\n",
        " [ 6.5  3.2]\n",
        " [ 5.   3.6]\n",
        " [ 6.   2.2]\n",
        " [ 6.4  3.1]\n",
        " [ 6.5  3. ]\n",
        " [ 5.   2. ]\n",
        " [ 6.1  3. ]\n",
        " [ 5.4  3.4]\n",
        " [ 6.4  2.8]\n",
        " [ 5.8  2.8]\n",
        " [ 4.7  3.2]\n",
        " [ 6.7  2.5]\n",
        " [ 7.3  2.9]\n",
        " [ 7.2  3.2]\n",
        " [ 5.5  2.4]\n",
        " [ 4.8  3. ]\n",
        " [ 4.8  3.4]\n",
        " [ 5.1  3.8]\n",
        " [ 5.1  3.3]\n",
        " [ 6.7  3. ]\n",
        " [ 5.6  2.8]\n",
        " [ 5.   3. ]\n",
        " [ 6.1  2.8]\n",
        " [ 4.6  3.4]\n",
        " [ 5.9  3. ]\n",
        " [ 6.   2.9]\n",
        " [ 5.6  2.9]\n",
        " [ 5.1  3.5]\n",
        " [ 6.7  3.1]\n",
        " [ 4.8  3.4]\n",
        " [ 6.6  3. ]\n",
        " [ 5.2  3.5]\n",
        " [ 4.4  3.2]\n",
        " [ 6.3  3.4]\n",
        " [ 6.9  3.2]\n",
        " [ 7.7  3. ]\n",
        " [ 7.7  2.6]\n",
        " [ 4.6  3.6]\n",
        " [ 5.1  3.5]\n",
        " [ 4.4  3. ]\n",
        " [ 6.   2.2]\n",
        " [ 7.6  3. ]\n",
        " [ 6.3  2.3]\n",
        " [ 5.4  3.7]\n",
        " [ 5.5  2.4]\n",
        " [ 5.7  2.9]\n",
        " [ 4.9  3.1]\n",
        " [ 4.7  3.2]\n",
        " [ 4.6  3.2]\n",
        " [ 5.7  2.6]] [2 1 0 1 2 2 1 2 0 0 1 2 2 0 1 0 1 1 1 0 2 2 1 2 0 2 1 0 2 1 0 1 1 1 2 0 0\n",
        " 2 0 1 2 0 0 2 1 2 1 2 1 2 1 1 1 2 1 2 0 0 1 1 1 0 2 0 2 2 2 1 1 0 2 2 0 2\n",
        " 2 2 1 0 0 0 0 1 2 0 1 0 1 1 1 0 2 0 1 0 0 2 2 2 2 0 0 0 1 2 1 0 1 1 0 0 0\n",
        " 1]\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training Score\n",
      "logclf.score(X_train,Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 142,
       "text": [
        "0.7767857142857143"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test Score\n",
      "logclf.score(X_test,Y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 143,
       "text": [
        "0.71052631578947367"
       ]
      }
     ],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Actual Y Data:\n",
      "Y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 144,
       "text": [
        "array([1, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 2, 0, 0, 2, 2, 1, 1, 0, 1, 2, 0, 0,\n",
        "       2, 2, 1, 2, 1, 1, 2, 0, 1, 2, 0, 2, 0, 0, 1])"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predicted Y Data\n",
      "logclf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 145,
       "text": [
        "array([2, 2, 0, 1, 0, 0, 0, 2, 0, 1, 1, 2, 0, 0, 2, 2, 1, 0, 0, 1, 1, 0, 0,\n",
        "       2, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0, 2, 0, 0, 2])"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The following 3 graphs are components that we will combine in our makeplot graphics.\n",
      "\n",
      "* First we plot the decision boundary, followed by training and testing data points.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the decision boundary. For that, we will assign a color to each\n",
      "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "h=.1\n",
      "#Plot Using only the training data\n",
      "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "\n",
      "# Z is predicting the category of the grid based on our prediction algorithm\n",
      "Z = logclf.predict(np.c_[xx.ravel(), yy.ravel()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Put the result into a color plot\n",
      "Z = Z.reshape(xx.shape)\n",
      "pl.figure(1, figsize=(15, 11))\n",
      "pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 147,
       "text": [
        "<matplotlib.collections.QuadMesh at 0x1133b82d0>"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the TRAINING points\n",
      "pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "pl.xlabel('Sepal length',size='xx-large')\n",
      "pl.ylabel('Sepal width',size='xx-large')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 148,
       "text": [
        "<matplotlib.text.Text at 0x114edf450>"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " # Plot the TESTING points\n",
      "pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='y', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "pl.xlabel('Sepal length')\n",
      "pl.ylabel('Sepal width')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 149,
       "text": [
        "<matplotlib.text.Text at 0x114edf450>"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Plotting Functions\n",
      "_Note that these could be made much cleaner.  They should be transformed into a single plotting function along with variables for classification in put, test/train/both, multiplotting, etc. (maybe a good way to contribute to sklearn?)_"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h = .01 # step size in the mesh for visualizing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.pcolormesh?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## PLOT JUST THE TRAINING SET\n",
      "def makeplot_train(classifier):\n",
      "        # Plot the decision boundary. For that, we will assign a color to each\n",
      "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    #Using only the training data\n",
      "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "    \n",
      "    # Create a meshgrid -- see what happens when h is smaller...\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "    \n",
      "    # Use classifier predictions for our point color\n",
      "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "   \n",
      "    # Put the result into a color plot for our background \n",
      "    Z = Z.reshape(xx.shape)\n",
      "    pl.figure(1, figsize=(15, 11))\n",
      "    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "    \n",
      "\n",
      "   \n",
      "    \n",
      "    # Plot the TRAINING points\n",
      "    pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "    \n",
      "     # Plot the TESTING points\n",
      "   # pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='y', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "    #pl.xlabel('Sepal length')\n",
      "   # pl.ylabel('Sepal width')\n",
      "\n",
      "    pl.xlim(xx.min(), xx.max())\n",
      "    pl.ylim(yy.min(), yy.max())\n",
      "    pl.xticks(())\n",
      "    pl.yticks(())\n",
      "    pl.title(classifier.__module__)\n",
      "    pl.size(5)\n",
      "\n",
      "    pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## PLOT JUST THE TEST SET WITH TRAINING BOUNDARIES\n",
      "def makeplot_test(classifier):\n",
      "        # Plot the decision boundary. For that, we will assign a color to each\n",
      "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    #Using only the training data\n",
      "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "   \n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    pl.figure(1, figsize=(15, 11))\n",
      "    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "\n",
      "   \n",
      "    \n",
      "    # Plot the TRAINING points\n",
      "   # pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "    #pl.xlabel('Sepal length')\n",
      "    #pl.ylabel('Sepal width')\n",
      "    \n",
      "    #Plot the TESTING points\n",
      "    pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='k', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "\n",
      "    pl.xlim(xx.min(), xx.max())\n",
      "    pl.ylim(yy.min(), yy.max())\n",
      "    pl.xticks(())\n",
      "    pl.yticks(())\n",
      "    pl.title(classifier.__module__)\n",
      "    pl.size(5)\n",
      "\n",
      "    pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeplot_both(classifier):\n",
      "        # Plot the decision boundary. For that, we will assign a color to each\n",
      "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    \n",
      "    #Plot Using only the training data\n",
      "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "   \n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    pl.figure(1, figsize=(15, 11))\n",
      "    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "\n",
      "   \n",
      "    \n",
      "    # Plot the TRAINING points\n",
      "    pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "    \n",
      "     # Plot the TESTING points\n",
      "    pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='y', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "\n",
      "    pl.xlim(xx.min(), xx.max())\n",
      "    pl.ylim(yy.min(), yy.max())\n",
      "    pl.xticks(())\n",
      "    pl.yticks(())\n",
      "    pl.title(classifier.__module__)\n",
      "    pl.size(5)\n",
      "\n",
      "    pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Plot the models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combined Using the \n",
      "logreg_fitter = linear_model.LogisticRegression()\n",
      "logclf=logreg_fitter.fit(X_train, Y_train)\n",
      "\n",
      "makeplot_train(logclf)\n",
      "print \"Accuracy Score of the Trained Classifier:\",logclf.score(X_train,Y_train)\n",
      "\n",
      "makeplot_test(logclf)\n",
      "print \"Accuracy Score of the Trained Classifier on the Test Set:\",logclf.score(X_test,Y_test)\n",
      "\n",
      "makeplot_both(logclf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy Score of the Trained Classifier: 0.776785714286\n",
        "Accuracy Score of the Trained Classifier on the Test Set:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.710526315789\n"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Your turn: Explore various classifiers on the same data and compare outputs\n",
      "\n",
      "KNN, Naive Bayes, Decision Trees -- how do they compare visually and in accuracy?\n",
      "\n",
      "How do changes in the test ratio change the accuracy and outputs?\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### A few additional examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pylab as pl\n",
      "from sklearn import linear_model, datasets\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "neigh = KNeighborsClassifier(n_neighbors=3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg_fitter = linear_model.LogisticRegression(C=1e5)\n",
      "nb_fitter = MultinomialNB()\n",
      "dt_fitter = DecisionTreeClassifier()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combined KNN\n",
      "\n",
      "KNN = KNeighborsClassifier(n_neighbors=5)\n",
      "KNNclf=KNN.fit(X_train, Y_train)\n",
      "\n",
      "makeplot_train(KNNclf)\n",
      "print \"Accuracy Score of the Trained Classifier:\",KNNclf.score(X_train,Y_train)\n",
      "\n",
      "makeplot_test(KNNclf)\n",
      "print \"Accuracy Score of the Trained Classifier on the Test Set:\",KNNclf.score(X_test,Y_test)\n",
      "\n",
      "makeplot_both(KNNclf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy Score of the Trained Classifier: 0.883928571429\n",
        "Accuracy Score of the Trained Classifier on the Test Set:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.710526315789\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combined DECISION TREES\n",
      "dt_fitter = DecisionTreeClassifier(max_depth=20)\n",
      "dtclf=dt_fitter.fit(X_train, Y_train)\n",
      "\n",
      "makeplot_train(dtclf)\n",
      "print \"Accuracy Score of the Trained Classifier:\",dtclf.score(X_train,Y_train)\n",
      "\n",
      "makeplot_test(dtclf)\n",
      "print \"Accuracy Score of the Trained Classifier on the Test Set:\" dtclf.score(X_test,Y_test)\n",
      "\n",
      "makeplot_both(dtclf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-160-5193f327f4ec>, line 9)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-160-5193f327f4ec>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    print \"Accuracy Score of the Trained Classifier on the Test Set:\" dtclf.score(X_test,Y_test)\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 160
    }
   ],
   "metadata": {}
  }
 ]
}