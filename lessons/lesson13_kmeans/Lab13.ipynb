{
 "metadata": {
  "name": "",
  "signature": "sha256:a6fd409260cf9900bcc88ab7863dceb247aed13ba9a7207507fa30246d790ee6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "K-Means"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 0 (location, location, location):\n",
      "\n",
      "copy this and the other notebooks to lab_submissions/lab13/flastname/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1 (understanding the basic algorithm):\n",
      "\n",
      "Let's make sure you understand exactly what k-means does."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell and study the images\n",
      "from IPython.core.display import HTML\n",
      "\n",
      "HTML('<iframe src=http://shabal.in/visuals/kmeans/1.html width=850px height= 800px></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### interactive widget\n",
      "[Play with this link](http://www.naftaliharris.com/blog/visualizing-k-means-clustering/) until you feel sure that you understand the two steps involved.\n",
      "\n",
      "If you want to challenge yourself, implement k-means yourself to really solidify how it works."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "from sklearn.metrics.pairwise import euclidean_distances\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# optional: implement k-means\n",
      "class KMeans():\n",
      "    def __init__(self, n_clusters=3):\n",
      "        '''\n",
      "        search for n_clusters\n",
      "        '''\n",
      "        self.labels_ = []\n",
      "        self.cluster_centers_ = []\n",
      "        self.n_clusters = n_clusters\n",
      "\n",
      "    def fit(self, data):\n",
      "        '''\n",
      "        find the clusters given data.\n",
      "        The skeleton below will give you a rudimentary and fragile algorithm.\n",
      "        '''\n",
      "        assert data.ndim == 2, 'data must be a 2-d numpy array'\n",
      "        # fill in code\n",
      "        \n",
      "        # first, pick three random points from the data and call these cluster centers\n",
      "        \n",
      "        # second, for every datum, assign to it the cluster center that's nearest to it.\n",
      "        \n",
      "        # third, find the centroids of the groups of points belonging to each cluster.\n",
      "        # Call those the new cluster centers.\n",
      "        \n",
      "        # Repeat steps 2 and 3 until the cluster assignments don't change.\n",
      "        \n",
      "        # store the assignments in self.labels_ and the the cluster centers in self.cluster_centers_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test your code by running this cell:\n",
      "\n",
      "# Generate sample data\n",
      "np.random.seed(0)\n",
      "\n",
      "centres = [[1, 1], [-0.5, 0], [1, -1]]\n",
      "X, labels_true = make_blobs(n_samples=1000, centers=centres, cluster_std=[[0.25,0.25]])\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "colors = ['r','b','g']\n",
      "for k, col in zip(range(3), colors):\n",
      "    my_members = labels_true == k\n",
      "    cluster_center = centres[k]\n",
      "    plt.scatter(X[my_members, 0], X[my_members, 1], c=col, marker='o',s=20) \n",
      "    plt.scatter(cluster_center[0], cluster_center[1], c=col, marker='o', s=200)\n",
      "plt.show()\n",
      "\n",
      "# fit data\n",
      "k_means_3 = KMeans(n_clusters=3)\n",
      "k_means_3.fit(X)\n",
      "k_means_3_labels = k_means_3.labels_\n",
      "k_means_3_cluster_centres = k_means_3.cluster_centers_\n",
      "\n",
      "# plot the outcome\n",
      "distance = euclidean_distances(k_means_3_cluster_centres,\n",
      "                               centres,\n",
      "                               squared=True)\n",
      "order = distance.argmin(axis=0)\n",
      "\n",
      "# KMeans 3\n",
      "plt.figure(figsize=(10, 10))\n",
      "for k, col in zip(range(3), colors):\n",
      "    my_wrong_members = (k_means_3_labels == order[k]) & (labels_true != k)  \n",
      "    plt.scatter(X[my_wrong_members, 0], X[my_wrong_members, 1],c = 'k',marker='x', s=200)               \n",
      "    my_members = k_means_3_labels == order[k]\n",
      "    plt.scatter(X[my_members, 0], X[my_members, 1],c=col, marker='o', s=20)           \n",
      "    cluster_center = k_means_3_cluster_centres[order[k]]\n",
      "    plt.scatter(cluster_center[0], cluster_center[1], marker = 'o', c=col, s=200, alpha=0.8)            \n",
      "    plt.scatter(centres[k][0], centres[k][1], marker ='o', c=col, s=200, alpha=0.8)\n",
      "             \n",
      "plt.title('KMeans 3')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 2 (exploring limits):\n",
      "\n",
      "Go through [this notebook](GA13_K-means_ErrorsAndDimensions.ipynb) for a quick survey of the situations where k-means can work well and when it can get confused. There is an optional exercise in there. If the answer is obvious, don't bother. But if you're not sure how to accomplish it, you might want to take the time to find out."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 3 (application):\n",
      "\n",
      "The pixels in an image can be considered as individual records that live in a 3D feature space (red, green, blue). You can try reducing the 16.7M colors into a just a few colors that are relevant to the particular image. You can do this yourself in [this notebook](GA13_KMeans_ImageCompression.ipynb).\n",
      "\n",
      "(Totally unrelated to k-means: a wonderful [exploration of digital color](http://codegolf.stackexchange.com/questions/22144/images-with-all-colors))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 4 (more application; optional):\n",
      "\n",
      "I didn't have time to clean up this notebook so it's a little rough. But if you consider the pixels of a greyscale image as individual features whose values are the intensities, and individual images as separate records (this is different from Part 3), then you can do different sorts of clustering. Check out [this example](GA13_OCR_Demo.ipynb) of using k-means for computer vision (optical character recognition/OCR)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 5 (alternatives):\n",
      "\n",
      "![](http://scikit-learn.org/stable/_images/plot_cluster_comparison_0011.png)\n",
      "\n",
      "The [sklearn docs](http://scikit-learn.org/stable/modules/clustering.html) has a very nice summary of many clustering algorithms including k-means. Pick a couple alternative clustering algorithms and describe how they differ from k-means."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### compose your treatise in this cell."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 6 (optional):\n",
      "\n",
      "What do you think of [this paper](http://machinelearning.wustl.edu/mlpapers/paper_files/LT17.pdf)? Just reading the abstract will get you far enough to respond."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}