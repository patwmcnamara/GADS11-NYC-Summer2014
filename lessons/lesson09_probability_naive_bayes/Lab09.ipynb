{
 "metadata": {
  "name": "",
  "signature": "sha256:d838d6433297f2b0eb977a29a96619fbdd0ed92ea6f1405134dcd0f48c7c275c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# If you aren't running this from `lab_submissions/lab08/$FLASTNAME` then copy it over and start again."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Probability\n",
      "\n",
      "We're in search of the perfect soft-boiled egg. The eggs are from different sources, and we don't have a timer. So the brand of the egg and the length of time they boil are random."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Joint Probability\n",
      "\n",
      "In words, what does $P(brand=Acme, time=optimal)$ refer to?\n",
      "\n",
      "Is it the same as $P(brand=Acme)$ or $P(time=optimal)$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conditional Probability\n",
      "\n",
      "In words, what does $P(time=optimal | brand=Acme)$ refer to?\n",
      "\n",
      "Is it the same as $P(brand=Acme | time=optimal)$?  yes/no\n",
      "\n",
      "Is it the same as $P(brand=Acme, time=optimal)$?  yes/no\n",
      "\n",
      "What is the relationship between the conditional and the joint probabilities?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Independence\n",
      "\n",
      "It's not unreasonable to think the optimal boil time has nothing to do with the brand of the egg. In this case, how do the conditional probabilities change?\n",
      "\n",
      "What is now the expression for the joint probability in terms of the new, simplified joint probabilities?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayes's Theorem\n",
      "\n",
      "Let's assume the different egg brands actually do have different ideal boil times. i.e. let's drop the independence assumption.\n",
      "\n",
      "We've boiled 100 eggs now. When we ate one that was cooked ideally, we took the pains to test the brand of the egg and found that of the 10 ideal eggs, 5 were Acme. We also know that we have 5 brands in our basket.\n",
      "\n",
      "Since Acme eggs seemed to perform well, we bought a bunch of Acme eggs. Given that we know we're about to crack open an Acme egg, what is a reasonable estimation of the probability that the egg is boiled ideally? What is $P(time=optimal | brand=Acme)$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Optional: Proof\n",
      "\n",
      "Starting with the equation in the slides, prove that the independence assumption leads directly to the final form. i.e., show that $P(x_1, x_2, ..., x_n | C)$ leads to $P(x_1 | C) \\cdot P(x_2 | C) \\cdot P(x_3 | C) \\cdot \\dots P(x_n | C)$\n",
      "\n",
      "Show work here:\n",
      "\n",
      "...\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The algorithm\n",
      "\n",
      "Recall that Bayes' Theorem provides that the _posterior_ probability of having a class $C$ given observations $\\{x\\}$ is given by the _likelihood_ of those observations given the class times the _prior_ probability of the class divided by the _evidence_. All together:\n",
      "\n",
      "$$\n",
      "posterior = {{likelihood \\cdot prior} \\over {evidence}} = P(C | \\{x\\}) = {{P(\\{x\\} | C) \\cdot P(C)} \\over {P(\\{x\\})}}\n",
      "$$\n",
      "\n",
      "You worked out an example just above where the class was whether the egg was cooked perfectly, and the evidence was the brand of the egg. (By the way, the evidence is over the training set (the basket) and not over the condition. P(acme) != 1. tricky tricky.) But if we want to include many, many features, the likelihood can quickly become impractical to measure. If we're using 10 features that are either true or false, we need to find $2^{10} = 1024$ likelihoods $P(x_1, x_2,...x_{10} | C)$ for each class. This means we need data that covers all of these combinations multiple times so that we can calculate meaningful probabilities.\n",
      "\n",
      "Naive Bayes lets avoid that combinatoric explosion. 10 features? 10 likelihoods. n features? n likelihoods. The expression above becomes:\n",
      "\n",
      "$$\n",
      "P(C | x_1, x_2,...x_n) = {{P(x_1, x_2,...x_n | C) \\cdot P(C)} \\over {P(\\{x\\})}} = {{P(x_1 | C) \\cdot P(x_2 | C) \\cdot P(x_3 | C) \\cdot \\dots \\cdot P(x_n | C) \\cdot P(C)} \\over {P(\\{x\\})}}\n",
      "$$\n",
      "\n",
      "The evidence term looks like it's still subject to the same problem, but we'll see why it's exempt.\n",
      "\n",
      "Recall in the egg example, you found the probability the egg was done right given that you know/observed the brand of the egg. The typical NB problem is slightly different. Given a new observation, what class does it belong to? What's the $C$?\n",
      "\n",
      "NB gives us the posterior $P(C|\\{x\\})$ for a particular class. So to answer this, you can find the posterior for all classes $C$, and the one with the largest probabiity wins. This is called the _Maximum A Posteriori_ (MAP) estimate. Formally, it's stated as:\n",
      "\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "C_{MAP} = \\argmax_C {{P(\\{x\\}) | C) \\cdot P(C)} \\over {P(\\{x\\})}} = \\argmax_C {P(\\{x\\}) | C) \\cdot P(C)}\n",
      "$$\n",
      "\n",
      "Argmax means: the value of $C$ that gives the maximum value. Since the evidence term does not depend on $C$, it's just a constant factor and doesn't change the argmax. So we can safely ignore it.\n",
      "\n",
      "All together, we find that the NB MAP estimate for what class to assign to our observation of $n$ features $\\{x_\\}$ is:\n",
      "\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "C_{MAP} = \\argmax_C {P(\\{x\\}) | C) \\cdot P(C)} = \\argmax_C {P(x_1 | C) \\cdot P(x_2 | C) \\cdot P(x_3 | C) \\cdot \\dots \\cdot P(x_n | C) \\cdot P(C)}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multinomial vs. Bernoulli NB:\n",
      "\n",
      "Wonderful. We have a tractable, simple algorithm for making categorical predictions based on prior observations. As always, the details get tricky. In particular, how do you encode features? i.e. what are the $P(x_i|C)$?\n",
      "\n",
      "Let's study a very common use of NB classifiers: document classification. Let's say we want to take all the world news articles on the NYTimes and group them into articles about individual countries. This means we have one class for every country. But what are $x_i$ or $P(x_i| C)$? There are two ways we can model this, but in each case the features correspond to unique words that appear in the documents.\n",
      "\n",
      "![](http://note.io/1ftU7X6)\n",
      "\n",
      "The **Multinomial Model** counts occurrences out of all possible occurrences for probability - better for greater features\n",
      "\n",
      "The diagram above translates to:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Beijing|C) \\cdot P(and|C) \\cdot P(Taipei|C) \\cdot P(join|C) \\cdot P(WTO|C) \\cdot P(C)$$\n",
      "\n",
      "Generally speaking, multinomial NB is given by:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C \\Pi_j P(word_j|C) \\cdot P(C) \\qquad word_j = \\text{word j in the document to be classified}\n",
      "\\\\\n",
      "P(word_j|C) = {{\\text{# of times word j occurs in all the training documents of class C}} \\over {\\text{# of (nonunique) words in all the training documents of class C}}}\n",
      "$$\n",
      "\n",
      "Thus the likelihood expands to as many terms as there are (nonunique) words **in the document** you want to classify.\n",
      "\n",
      "The **Bernoulli model** counts only all documents with presence of the word - better for fewer features\n",
      "\n",
      "The diagram above translates to:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Alaska=0|C) \\cdot P(Beijing=1|C) \\cdot P(India=0|C) \\cdot P(join=1|C) \\cdot P(Taipei=1|C) \\cdot P(WTO=1|C) \\cdot P(C)$$\n",
      "\n",
      "Generally speaking, Bernoulli NB is given by:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C \\Pi_k P(word_k|C) \\cdot P(C) \\qquad word_k = \\text{whether or not word k in the training set is in the document to be classified}\n",
      "\\\\\n",
      "P(word_k|C) = {{\\text{# of documents of class C that contain word k}} \\over {\\text{# documents of class C in the training set}}}\n",
      "$$\n",
      "\n",
      "Thus the likelihood expands to as many terms as there are unique words **in the training set documents** for a class."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Comparison\n",
      "\n",
      "The number of likelihoods that have to be calculated in each case is the same: the number of unique words in the training set for each class. The multinomial model takes fewer terms to classify a new document than the Bernoulli model does. But the Bernoulli model takes into account more information. Sometimes the absence of a word is as telling as the presence of another."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## More\n",
      "\n",
      "You may be wondering what happens when the new document contains a word that was never seen in the training set. Its likelihood would be zero, and that would nullify all prior knowledge about the words in the class. If this is unsettling, you can read more about [additive smoothing](http://en.wikipedia.org/wiki/Additive_smoothing), also called Laplace smoothing. In the sklearn implementation, the $\\alpha$ parameter defaults to one, which is called add-one smoothing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Demo Code\n",
      "\n",
      "Let's start by seeing how scikit learn converts text into features that we can use for Naive Bayes classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB  # Naive Bayes model classes\n",
      "from sklearn.feature_extraction.text import CountVectorizer  # convert text into feature vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "### How the Count Vectorizer Works -- Just run and read for the next few cells\n",
      "#\n",
      "\n",
      "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']\n",
      "print \"Original text is:\"\n",
      "for i, doc in enumerate(text):\n",
      "    print i, ':', doc\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "print\n",
      "print 'features'\n",
      "for i, feature in enumerate(vectorizer.get_feature_names()):\n",
      "    print i, feature\n",
      "\n",
      "# call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)\n",
      "print\n",
      "print '(document, feature)  count'\n",
      "print x\n",
      "print\n",
      "print 'the same data in matrix form:'\n",
      "print x.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original text is:\n",
        "Math is great\n",
        "Math is really great\n",
        "Exciting exciting Math\n",
        "\n",
        "features\n",
        "[(0, u'exciting'), (1, u'great'), (2, u'is'), (3, u'math'), (4, u'really')]\n",
        "\n",
        "(document, feature)  count\n",
        "  (0, 1)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 3)\t1\n",
        "  (1, 1)\t1\n",
        "  (1, 2)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 3)\t1\n",
        "\n",
        "the same data in matrix form:\n",
        "[[0 1 1 1 0]\n",
        " [0 1 1 1 1]\n",
        " [2 0 0 1 0]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The vectorizer treats each word as a feature, and we call this the 'bag of words' approach. Notice that the bag of words treatment doesn't preserve information about the *order* of words, just their frequency. Since generally most documents do not contain most words, the counts are stored in a sparse format. So `(0, 1)` means that document 0 ('Math is great') contains word 1 ('great') once. `(2, 0)` means document 2 ('Exciting exciting math') contains word 0 ('exciting') twice.\n",
      "\n",
      "The matrix shows the same information, where each row represents a document, each column represents a feature/word, and the element values represent the counts of those features/words in those documents."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## optional: calculate likelihoods\n",
      "Let's pretend these documents are all from one class. From the definitions above, what are the multinomial and Bernoulli probabilities for each of these words? You can do this by hand or by code.\n",
      "\n",
      "Hint: $P_{m}(exciting) = 0.2$, $P_{B}(exciting) = 0.33$\n",
      "\n",
      "If you really want to try implementing the CountVectorizer, go for it, but I don't think you'll learn much."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## other features\n",
      "\n",
      "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) allows you to set a few parameters. `binary` and `ngram_range` are relevant here. `binary` ignores the count of a feature in a document, and only returns a 1 if the feature is present at all. n-grams are groups of n adjacent words. The bag of words model is a unigram/1-gram model. You can specify the shortest and longest n-grams to include in your model.\n",
      "\n",
      "Side note: Google [provides](https://books.google.com/ngrams) a huge data set of ngrams from their books project spanning 200 years!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can include bigrams and larger in your model as well\n",
      "vectorizer = CountVectorizer(binary=True)\n",
      "\n",
      "# fit_transform does fit and transform on the same data\n",
      "x = vectorizer.fit_transform(text)\n",
      "print '## binary. cf. before ##'\n",
      "print 'features'\n",
      "for i, feature in enumerate(vectorizer.get_feature_names()):\n",
      "    print i, feature\n",
      "\n",
      "print '(document, feature)  count'\n",
      "print x\n",
      "\n",
      "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
      "\n",
      "# fit_transform does fit and transform on the same data\n",
      "x = vectorizer.fit_transform(text)\n",
      "print\n",
      "print '## unigrams and bigrams ##'\n",
      "print 'features'\n",
      "for i, feature in enumerate(vectorizer.get_feature_names()):\n",
      "    print i, feature\n",
      "\n",
      "print '(document, feature)  count'\n",
      "print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# English Wikipedia of Simple English Wikipedia?\n",
      "\n",
      "## Now let's fetch some real text samples.\n",
      "\n",
      "Little known fact: The greatest value of Wikipedia lies not in the incredibly broad collection of human knowledge, but in providing Machine Learning researchers with an easily accessible corpus with which to train their models.\n",
      "\n",
      "Just kidding. Sort of.\n",
      "\n",
      "We're going to fetch a bunch of documents from the [English](http://en.wikipedia.org/wiki/Data_science) and [Simple English](http://simple.wikipedia.org/wiki/Data_science) versions of Wikipedia. Wikipedia articles tend to be quite comprehensive, but also incredibly verbose. Simple was started to offer lay explanations without so much of the jargon so that children or non-native speakers can come and learn. We'll train our classifier on these documents and then see if it can guess whether new documents came from one or the other.\n",
      "\n",
      "The fun begins with building up a training set....\n",
      "\n",
      "### In your terminal, type `pip install wikipedia`. If that doesn't work, `sudo pip install wikipedia`.\n",
      "\n",
      "You can read the docs for this python wrapper here: https://wikipedia.readthedocs.org/en/latest/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import wikipedia as wiki  # wikipedia api wrapper\n",
      "wiki.set_rate_limiting(True)  # might actually speed things up.\n",
      "from sklearn.cross_validation import train_test_split  # split the data you have into training and test sets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named wikipedia",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-2f49ec932d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwikipedia\u001b[0m  \u001b[0;31m# wikipedia api wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rate_limiting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# might actually speed things up.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m  \u001b[0;31m# split the data you have into training and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named wikipedia"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I've defined a function for you to fetch articles. It's not necessary to understand how it works, but I've tried to make it as readable as possible."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fetch_wiki(title, lang):\n",
      "    '''\n",
      "    Return the regular English or simple versions of an article.\n",
      "    Simple versions are far shorter than the regular ones, so only\n",
      "    pull the summary of regular articles.\n",
      "    In case of an error, just return None instead of crashing the program.\n",
      "    '''\n",
      "    if lang not in ('en', 'simple'):\n",
      "        \"Language must be 'en' or 'simple'\"\n",
      "        return\n",
      "\n",
      "    try:\n",
      "        wiki.set_lang(lang)  # tells wikipedia whether to search the english or simple sites.\n",
      "        page = wiki.page(title)  # fetches a page matching the title from the desired wiki site.\n",
      "        # print page.title  # used for testing the function\n",
      "        if lang == 'en':\n",
      "            text = page.summary\n",
      "            flag = 1  # tells the classifier later on that this document came from English wiki\n",
      "        else:\n",
      "            text = page.content\n",
      "            flag = 0  # tells the classifier later on that this document came from Simple wiki\n",
      "        return (text, flag)\n",
      "\n",
      "    except:  # NOTE: you should never have a blind `except` like this. but, hey, we're hacking.\n",
      "        # The wiki.page method frequently errors out. In this case, print a notification and return None.\n",
      "        # without this, the whole program would halt entirely.\n",
      "        print 'error with ' + lang + ' page for: ' + title\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we use the function to fetch a sample of documents from both english and simple wiki."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = ['General Relativity', 'Bayes Theorem', 'Ada Lovelace',\n",
      "            'jackfruit', 'mantis shrimp', 'Der Ring des Nibelungen',\n",
      "            'antikythera mechanism', 'teflon', 'superconductor',\n",
      "            'Harper Lee', 'durian', 'Shostakovich', 'steel',\n",
      "            'database', 'transistor', 'Goethe', 'dog', 'meme', 'spleen',\n",
      "            'morphine', 'maple', \"rubik's cube\", 'souffle', 'chlorine',\n",
      "            'earthworm', 'prune', 'ballet', 'ultrasound', 'bruce lee']\n",
      "\n",
      "corpus = []\n",
      "\n",
      "# search for both versions of each article, and if we get a result, store it in the corpus\n",
      "# This will take a minute. The API is slow.\n",
      "for article in articles:\n",
      "    en = fetch_wiki(article, 'en')\n",
      "    if en:\n",
      "        corpus.append(en)\n",
      "    simple = fetch_wiki(article, 'simple')\n",
      "    if simple:\n",
      "        corpus.append(simple)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we convert the data that came out of my function into the vector representation that we introduced before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "\n",
      "# reorganize the corpus from: [(text1, flag1), (text2, flag2), ..., (textn, flag2)]\n",
      "# to: [(text1, text2, ..., textn), (flag1, flag2, ..., flagn)]\n",
      "text, Y = zip(*corpus)\n",
      "\n",
      "# this pattern is equivalent to, but much faster and more general than:\n",
      "text, Y = [], []\n",
      "for pair in corpus:\n",
      "    text.append(pair[0])\n",
      "    Y.append(pair[1])\n",
      "text = tuple(text)\n",
      "Y = tuple(Y)\n",
      "\n",
      "# this zip(*sequence) trick comes in really handy in a lot of places, so it's a good one to know and understand\n",
      "# https://docs.python.org/2/library/functions.html#zip\n",
      "\n",
      "# convert the text into a sparse vector representation as before\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we use SKLearn's [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) function. By default, 75% of the records, `X`,  and their labels, `Y`, are randomly assigned to `train`, and the remaining 25% are assigned to `test`. We'll use the `train` samples to fit our model and the `test` samples to evaluate how well it performs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally we use scikit learn's Naive Bayes models to fit the training, then we score them on accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Optional:\n",
      "\n",
      "If you're especially eager, you may choose to write your own implementation. Here's a bit of a skeleton class to give you a similar API as the scikit learn classes. I will caution that you should definitely understand how to implement add-one smoothing, otherwise it's guaranteed to overfit and will fail on new data. Also, CountVectorizer outputs an unusual [representation](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html). I recommend using `.toarray()` and working with that familiar numpy representation.\n",
      "\n",
      "Another warning: with enough features, it's very likely you'll run into [floating point underflow](http://en.wikipedia.org/wiki/Arithmetic_underflow) conditions where the probabilities become smaller than a floating point represenation can store. If this happens, the only change necessary is to store the logs of the probabilities, and add these together instead of multiplying them. Everything else remains the same."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# totally optional!\n",
      "\n",
      "class MyNB():\n",
      "    def __init__(self):\n",
      "        '''store the model parameters'''\n",
      "        self.likelihoods =\n",
      "        self.priors =\n",
      "\n",
      "    def fit(self, X, Y):\n",
      "        \n",
      "        # convert X into a usable representation\n",
      "        X = X.toarray()\n",
      "        # as an array, the first index iterates through documents while the second iterates through words.\n",
      "        \n",
      "        # calculate and store priors using Y\n",
      "        # you may use the knowledge that Y is either 0 or 1\n",
      "        \n",
      "        # calculate and store likelihoods using X and Y\n",
      "        \n",
      "        return\n",
      "\n",
      "    def predict(self, X):\n",
      "        \n",
      "        # keep track of the predictions\n",
      "        output = []\n",
      "        \n",
      "        # loop through the documents in X\n",
      "        for document_features in X.toarray():\n",
      "            # keep track of the posterior for each class\n",
      "            posteriors = {}\n",
      "\n",
      "            # loop through classes. In this case, 0 or 1\n",
      "            for y in (0, 1):\n",
      "                # fetch the relevant priors and likelihoods and multiply them all together\n",
      "\n",
      "                # store\n",
      "                posteriors[y] = posterior\n",
      "            \n",
      "            # store the prediction for this document\n",
      "            output.append( # make prediction )\n",
      "        \n",
      "        return output\n",
      "        \n",
      "    def score(self, X, Y):\n",
      "        return np.mean(self.predict(X) == Y)  # this matches what sklearn does."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# More data\n",
      "\n",
      "That didn't work so well. You can repeat the scoring cell a few times to see much the accuracy depends on the train/test split. This calls for more data.\n",
      "\n",
      "Use the `.random(n_titles)` method to retrieve a list of 100 more titles and fetch the text for training. Note that `.random` will only return up to 10 titles at a time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki.set_lang('simple')\n",
      "\n",
      "for i in xrange(10):\n",
      "    print i  # monitor progress -- this is really slow\n",
      "    for title in wiki.random(10):\n",
      "        # your turn! Find each title in each language in wiki and if you get a result, store it to corpus."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We just spent a long time fetching this data, so let's save it so we don't have to do it all over again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(corpus, open('corpus.p', 'wb'))\n",
      "\n",
      "# restore with: corpus = pickle.load(open('corpus.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How are we doing now?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "text, Y = zip(*corpus)\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Experiment\n",
      "\n",
      "The variance is a bit contained, but it's still not performing too well. Now it's your turn to try a few things to see if you can improve the performance to something useful. You can try fetching even more data overnight. We'd only fetched the English article summaries, but maybe it'll help to use the full articles. You can try filtering out words < 3 or 4 chars long. We haven't tried allowing bigrams and trigrams yet or removing stop-words (words like 'an', or 'of' that provide no meaning). Both the Multinomial and Bernoulli models have model parameters that you can try adjusting. The alpha parameter turns out to be really helpful. Finally, you might imagine other ways to tweak our model!\n",
      "\n",
      "Here are links to relevant docs:\n",
      "\n",
      "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
      "[MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
      "[BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# start here!\n",
      "\n",
      "vectorizer = CountVectorizer( # try setting options in here )\n",
      "X = vectorizer.fit_transform(text)\n",
      "\n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "clf = MultinomialNB( # try setting options in here ).fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB( # try setting options in here ).fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}