{
 "metadata": {
  "name": "",
  "signature": "sha256:7c84bac44faadb0bf4fecb325a3c6352e80eec39cb8e623c7fc7d158a3e7a0cb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sentiment Analysis\n",
      "\n",
      "Let's make some money.\n",
      "\n",
      "For our training set, we'll use the Rotten Tomatoes reviews from before. We'll start by using a logistic regression model as our classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data\n",
      "# the try block should work from your lab submission folder. The except block works from the lesson folder.\n",
      "\n",
      "try:\n",
      "    df = pd.read_csv('../../../lessons/lesson09_probability_naive_bayes/rt_critics.csv')\n",
      "except:\n",
      "    df = pd.read_csv('../lesson09_probability_naive_bayes/rt_critics.csv')\n",
      "\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to challenge yourself, ignore the subsequent cells and create the classifier on your own with your favorite model right here.\n",
      "\n",
      "See how stop words and tf-idf scoring helps or hurts your model.\n",
      "\n",
      "When you're done with that, skip to 'Next Steps'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# play space for the bold.\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell to examine data\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "Xcv = vectorizer.fit_transform(df['quote'])\n",
      "\n",
      "print '%d samples, %d features' % Xcv.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But wait! We have more features than samples. This would ensure overfitting. Let's trim that number down to the top 5000, ranked by the term frequency across all documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell to vectorize our documents\n",
      "\n",
      "# create vectorizer object\n",
      "vectorizer = CountVectorizer(max_features=5000)\n",
      "\n",
      "# convert our documents and their labels into numpy arrays\n",
      "Xcv = vectorizer.fit_transform(df['quote'])\n",
      "Y = (df['fresh'] == 'fresh').values.astype(np.int8)\n",
      "\n",
      "# split the converted data into training and test sets\n",
      "xtrain, xtest, ytrain, ytest = train_test_split(Xcv, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Evaluate performance of models\n",
      "# Tune the regularization parameter, C, to improve performance.\n",
      "\n",
      "logr = LogisticRegression(C=1.0).fit(xtrain, ytrain)  # Set the keyword argument, 'C', to different values and optimize\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * logr.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Stop Words\n",
      "\n",
      "The performance isn't horrible, but it's not great. Can we improve things by [using stop words](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)? See the linked documentation for how to tell CountVectorizer to skip stop words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# edit this cell to include stopwords\n",
      "\n",
      "# create vectorizer object\n",
      "vectorizer = CountVectorizer(max_features=5000)\n",
      "\n",
      "# convert our documents and their labels into numpy arrays\n",
      "Xcvs = vectorizer.fit_transform(df['quote'])\n",
      "Y = (df['fresh'] == 'fresh').values.astype(np.int8)\n",
      "\n",
      "# split the converted data into training and test sets\n",
      "xtraincvs, xtestcvs, ytraincvs, ytestcvs = train_test_split(Xcvs, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Evaluate performance of models\n",
      "# Tune the regularization parameter, C, to improve performance.\n",
      "\n",
      "logr = LogisticRegression(C=1.0).fit(xtraincvs, ytraincvs)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * logr.score(xtestcvs, ytestcvs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# tf-idf\n",
      "\n",
      "If that didn't work, how about using tf-idf weighting?\n",
      "\n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# edit this cell to create a TfidfVectorizer instead of a simple CountVectorizer\n",
      "\n",
      "# create vectorizer object\n",
      "vectorizer = CountVectorizer(max_features=5000)\n",
      "\n",
      "# convert our documents and their labels into numpy arrays\n",
      "Xti = vectorizer.fit_transform(df['quote'])\n",
      "Y = (df['fresh'] == 'fresh').values.astype(np.int8)\n",
      "\n",
      "# split the converted data into training and test sets\n",
      "xtrainti, xtestti, ytrainti, ytestti = train_test_split(Xti, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Evaluate performance of models\n",
      "# Tune the regularization parameter, C, to improve performance.\n",
      "\n",
      "logr = LogisticRegression(C=1.0).fit(xtrainti, ytrainti)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * logr.score(xtestti, ytestti))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# tf-idf and stop words\n",
      "\n",
      "Do both together help?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# edit this cell to create a TfidfVectorizer that uses stop words\n",
      "\n",
      "# create vectorizer object\n",
      "vectorizer = CountVectorizer(max_features=5000)\n",
      "\n",
      "# convert our documents and their labels into numpy arrays\n",
      "Xtis = vectorizer.fit_transform(df['quote'])\n",
      "Y = (df['fresh'] == 'fresh').values.astype(np.int8)\n",
      "\n",
      "# split the converted data into training and test sets\n",
      "xtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Evaluate performance of models\n",
      "# Tune the regularization parameter, C, to improve performance.\n",
      "\n",
      "logr = LogisticRegression(C=1.0).fit(xtraintis, ytraintis)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * logr.score(xtesttis, ytesttis))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Next steps\n",
      "\n",
      "Are you satisfied with these results? Why might you be less than satisfied? How can you explain the observed behavior? What are the next steps you would need to do to improve this classifier? If you have time remaining, try a few strategies out below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# continue playing here.\n",
      "# did you finish all of the previous labs? How do your implementations compare?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# More Next Steps\n",
      "\n",
      "We're not making any money with this classifier yet. If it were that easy, everyone would do it and there'd be no money in it. The hardest part of this problem is usually finding good training data. Googling 'sentiment analysis training data' or 'sentiment analysis test data' turns up a few freely available sources. Most of them are hosted by universities.\n",
      "\n",
      "But notice, determining the judgment of a movie review isn't the same task as determining the emotional content of a tweet. And yet, it kind of is. The computer doesn't know anything about nature of the text. All it knows is that there are documents with one label (fresh/happy) and documents with another label (rotten/sad) and it needs to fit a model to discriminate between the two. This can be extended to more classes (look into the 20 newsgroups dataset in sci-kit learn) and to proprietary corpora.\n",
      "\n",
      "One application you might use at work is classifying support emails from users. The classes may be 'ranting', 'mischarge', 'lost order', 'gushing'. Or whatever is common. Even if the classifier isn't perfect, it could help streamline the process of getting the right emails to the right support personnel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('''\n",
      "<style>\n",
      ".text_cell_render {\n",
      "  background-color: cyan;\n",
      "}\n",
      "</style>\n",
      "''')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "<style>\n",
        ".text_cell_render {\n",
        "  background-color: cyan;\n",
        "}\n",
        "</style>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x1108df2d0>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}