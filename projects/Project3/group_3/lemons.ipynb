{
 "metadata": {
  "name": "",
  "signature": "sha256:0a079ec1231f7228aaf19a8143cc4a240b03e575bb8b7dc6526c279bb206e941"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Define the Problem: Car is a good buy (0) or bad buy (1)\n",
      "Type of Problem: Classification problem\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "# IMPORT MODULES\n",
      "import pandas as pd\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "# IMPORT DATA\n",
      "train = pd.read_csv(\"lemon_training.csv\")\n",
      "test = pd.read_csv(\"lemon_test.csv\")\n",
      "\n",
      "# CLEAN DATA\n",
      "train.drop(['AUCGUART', 'PRIMEUNIT', 'BYRNO','RefId','WheelType','PurchDate'],axis=1, inplace=True)\n",
      "test.drop(['AUCGUART', 'PRIMEUNIT', 'BYRNO','WheelType','PurchDate'],axis=1, inplace=True)\n",
      "\n",
      "# Try custom fields\n",
      "train['km_year'] = train.VehOdo / (train.VehicleAge +1)\n",
      "test['km_year'] = test.VehOdo / (test.VehicleAge+1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data transforms\n",
      "# Appear not to make much difference to Random Forest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add in number of missing fields\n",
      "def flag_nulls(df_in):\n",
      "    df_in['missing_data'] = 0\n",
      "    df_in['missing_price'] = 0\n",
      "    for feature in df_in.columns:\n",
      "        df_in['missing_data'] = (df_in['missing_data']) + (pd.isnull(df_in[feature])).apply(lambda x: 1 if x else 0)\n",
      "        if feature in ['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice','MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice']:\n",
      "            df_in['missing_price'] = (df_in['missing_price']) + (df_in[feature]==0).apply(lambda x: 1 if x else 0)\n",
      "    return df_in      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Count number of missing fields\n",
      "test = flag_nulls(test)\n",
      "train = flag_nulls(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def imputation(df_in):\n",
      "    # Flag missing data\n",
      "    df_in['Trim'].replace(to_replace=np.NaN, value='missing_trim', inplace = True)\n",
      "    df_in['WheelTypeID'].replace(to_replace=np.NaN, value=max(train.WheelTypeID.values)+1, inplace = True)\n",
      "    df_in['Color'].replace(to_replace=np.NaN, value='missing_color', inplace = True)\n",
      "    df_in['SubModel'].replace(to_replace=np.NaN, value='missing_submodel', inplace = True)\n",
      "    df_in['Transmission'].replace(to_replace=np.NaN, value='missing_trainsmission', inplace = True)\n",
      "    df_in['Nationality'].replace(to_replace=np.NaN, value='missing_nationality', inplace = True)\n",
      "    df_in['Size'].replace(to_replace=np.NaN, value='missing_size', inplace = True)\n",
      "    df_in['TopThreeAmericanName'].replace(to_replace=np.NaN, value='missing_topthree', inplace = True)\n",
      "    \n",
      "    # Regress on Acquision prices to fill in Current prices\n",
      "    regress_flag = True\n",
      "    if regress_flag:\n",
      "        from sklearn import linear_model\n",
      "        from sklearn.pipeline import Pipeline\n",
      "        from sklearn.preprocessing import PolynomialFeatures\n",
      "        from sklearn.cross_validation import train_test_split\n",
      "\n",
      "        clf = linear_model.LinearRegression()\n",
      "        degree = 3\n",
      "        polynomial_features = PolynomialFeatures(degree=degree,include_bias=True)\n",
      "        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
      "                                 (\"classifier\", clf)])\n",
      "\n",
      "        non_null_train = df_in[(pd.notnull(df_in.MMRAcquisitionAuctionAveragePrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitionAuctionCleanPrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitionRetailAveragePrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitonRetailCleanPrice)) & \n",
      "                               (pd.notnull(df_in.MMRCurrentAuctionAveragePrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentAuctionCleanPrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentRetailAveragePrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentRetailCleanPrice))]\n",
      "\n",
      "        null_mask = ((pd.notnull(df_in.MMRAcquisitionAuctionAveragePrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitionAuctionCleanPrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitionRetailAveragePrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitonRetailCleanPrice)) & \n",
      "           (pd.isnull(df_in.MMRCurrentAuctionAveragePrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentAuctionCleanPrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentRetailAveragePrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentRetailCleanPrice)))\n",
      "\n",
      "        null_predict = df_in[null_mask]\n",
      "\n",
      "        X = non_null_train[['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice','VehicleAge', 'VehOdo']]\n",
      "        X_predict = null_predict[['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice','VehicleAge', 'VehOdo']]\n",
      "        for target in ['MMRCurrentAuctionAveragePrice','MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice','MMRCurrentRetailCleanPrice']:             \n",
      "            \n",
      "            y = non_null_train[target]\n",
      "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
      "            pipeline.fit(X_train, y_train)\n",
      "            print pipeline.score(X_train,y_train)\n",
      "            print pipeline.score(X_test,y_test)            \n",
      "            result = pipeline.predict(X_predict)\n",
      "            df_in.ix[null_mask,target] = result\n",
      "   \n",
      "    # Insert mean values into the few remaining rows\n",
      "    df_in['MMRAcquisitionAuctionAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionAuctionAveragePrice), inplace = True)\n",
      "    df_in['MMRAcquisitionAuctionCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionAuctionCleanPrice), inplace = True)\n",
      "    df_in['MMRAcquisitionRetailAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionRetailAveragePrice), inplace = True)\n",
      "    df_in['MMRAcquisitonRetailCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitonRetailCleanPrice), inplace = True)\n",
      "    df_in['MMRCurrentAuctionAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentAuctionAveragePrice), inplace = True)\n",
      "    df_in['MMRCurrentAuctionCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentAuctionCleanPrice), inplace = True)\n",
      "    df_in['MMRCurrentRetailAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentRetailAveragePrice), inplace = True)\n",
      "    df_in['MMRCurrentRetailCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentRetailCleanPrice), inplace = True)\n",
      "    return df_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Take care of missing data\n",
      "test = imputation(test)\n",
      "train = imputation(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.897037097246\n",
        "0.900372005312"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.889060104372"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.893864646167"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.860443969427"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.865313269768"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.850626829535"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.856441720286"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.893759529182"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.893521001697"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.887298553756"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.883117932702"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.860657812361"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.855791914533"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.853049733398"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.84420334788"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use all features\n",
      "features = list(train.columns)\n",
      "features.remove('IsBadBuy') "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# RefID\t\t\t\t        Unique (sequential) number assigned to vehicles\n",
      "# IsBadBuy\t\t\t\tIdentifies if the kicked vehicle was an avoidable purchase \n",
      "# PurchDate\t\t\t\tThe Date the vehicle was Purchased at Auction\n",
      "# Auction\t\t\t\t\tAuction provider at which the  vehicle was purchased\n",
      "# VehYear\t\t\t\t\tThe manufacturer's year of the vehicle\n",
      "# VehicleAge\t\t\t\tThe Years elapsed since the manufacturer's year\n",
      "# Make\t\t\t\t\tVehicle Manufacturer \n",
      "# Model\t\t\t\t\tVehicle Model\n",
      "# Trim\t\t\t\t\tVehicle Trim Level\n",
      "# SubModel\t\t\t\tVehicle Submodel\n",
      "# Color\t\t\t\t\tVehicle Color\n",
      "# Transmission\t\t\t\tVehicles transmission type (Automatic, Manual)\n",
      "# WheelTypeID\t\t\t\tThe type id of the vehicle wheel\n",
      "# WheelType\t\t\t\tThe vehicle wheel type description (Alloy, Covers)\n",
      "# VehOdo\t\t\t\t\tThe vehicles odometer reading\n",
      "# Nationality\t\t\t\tThe Manufacturer's country\n",
      "# Size\t\t\t\t\tThe size category of the vehicle (Compact, SUV, etc.)\n",
      "# TopThreeAmericanName\t\t\tIdentifies if the manufacturer is one of the top three American manufacturers\n",
      "# MMRAcquisitionAuctionAveragePrice\tAcquisition price for this vehicle in average condition at time of purchase\t\n",
      "# MMRAcquisitionAuctionCleanPrice\t\tAcquisition price for this vehicle in the above Average condition at time of purchase\n",
      "# MMRAcquisitionRetailAveragePrice\tAcquisition price for this vehicle in the retail market in average condition at time of purchase\n",
      "# MMRAcquisitonRetailCleanPrice\t\tAcquisition price for this vehicle in the retail market in above average condition at time of purchase\n",
      "# MMRCurrentAuctionAveragePrice\t\tAcquisition price for this vehicle in average condition as of current day\t\n",
      "# MMRCurrentAuctionCleanPrice\t\tAcquisition price for this vehicle in the above condition as of current day\n",
      "# MMRCurrentRetailAveragePrice\t\tAcquisition price for this vehicle in the retail market in average condition as of current day\n",
      "# MMRCurrentRetailCleanPrice\t\tAcquisition price for this vehicle in the retail market in above average condition as of current day\n",
      "# PRIMEUNIT\t\t\t\tIdentifies if the vehicle would have a higher demand than a standard purchase\n",
      "# AcquisitionType\t\t\t\tIdentifies how the vehicle was aquired (Auction buy, trade in, etc)\n",
      "# AUCGUART\t\t\t\tThe level guarntee provided by auction for the vehicle (Green light - Guaranteed/arbitratable, Yellow Light - caution/issue, red light - sold as is)\n",
      "# BYRNO\t\t\t\t\tUnique number assigned to the buyer that purchased the vehicle\n",
      "# VNZIP                                   Zipcode where the car was purchased\n",
      "# VNST                                    State where the the car was purchased\n",
      "# VehBCost\t\t\t\tAcquisition cost paid for the vehicle at time of purchase\n",
      "# IsOnlineSale\t\t\t\tIdentifies if the vehicle was originally purchased online\n",
      "# WarrantyCost                            Warranty price (term=36month  and millage=36K) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make dummy variables over entire train/test set\n",
      "def make_dummies(train_in, test_in, feature):\n",
      "    from sklearn import preprocessing\n",
      "    le = preprocessing.LabelEncoder()\n",
      "    train_set = list(set(train_in[feature].values))\n",
      "    test_set = list(set(test_in[feature].values))\n",
      "    encoder_set = list(set(train_set + test_set))\n",
      "    le.fit(encoder_set)\n",
      "    new_feature = le.transform(train_in[feature].values)\n",
      "    train_in=train_in.drop(feature, axis=1)\n",
      "    train_in[feature] = new_feature \n",
      "    new_feature = le.transform(test_in[feature].values)\n",
      "    test_in=test_in.drop(feature, axis=1)\n",
      "    test_in[feature] = new_feature\n",
      " \n",
      "    return test_in, train_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dummy all the things\n",
      "for feature in ['Size','VNST','TopThreeAmericanName','Nationality','Transmission','Color','SubModel','Trim','Model','Make','Auction']:\n",
      "    test, train = make_dummies(train, test, feature)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CREATE TRAINING & TEST DATA\n",
      "training_X = train[features].values\n",
      "training_y = train['IsBadBuy'].values\n",
      "\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "training_X = training_X.astype(float)\n",
      "training_y = training_y.astype(float)\n",
      "\n",
      "X_split1, X_split2, y_split1, y_split2 = train_test_split(training_X, training_y, test_size=0.35, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random forest performs among the best\n",
      "from sklearn.ensemble import RandomForestClassifier  \n",
      "\n",
      "RF = RandomForestClassifier (n_estimators=50,criterion='entropy',min_samples_split=285)\n",
      "RF.fit(X_split1,y_split1)\n",
      "y_predict1 = RF.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = RF.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)\n",
      "importance = zip(RF.feature_importances_, features)\n",
      "for rank in sorted(importance, key=lambda x: x[0], reverse=True):\n",
      "    print rank"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.387283236994\n",
        "Split2 F1: 0.374744027304\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.74      0.26      0.39      4093\n",
        "\n",
        "avg / total       0.88      0.90      0.88     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.75      0.25      0.37      2196\n",
        "\n",
        "avg / total       0.88      0.90      0.87     17881\n",
        "\n",
        "(0.21693400589357886, 'WheelTypeID')\n",
        "(0.20713670240466328, 'missing_data')\n",
        "(0.045617602289179354, 'VehicleAge')\n",
        "(0.038308851375575673, 'MMRCurrentAuctionAveragePrice')\n",
        "(0.038053517415815044, 'VehBCost')\n",
        "(0.036057175741676696, 'VehOdo')\n",
        "(0.033771816949598042, 'MMRCurrentAuctionCleanPrice')\n",
        "(0.033335798515746369, 'km_year')\n",
        "(0.032563377432198497, 'MMRAcquisitionAuctionCleanPrice')\n",
        "(0.031896847148079027, 'VehYear')\n",
        "(0.026926186750057091, 'MMRAcquisitionAuctionAveragePrice')\n",
        "(0.025259979673381162, 'MMRCurrentRetailAveragePrice')\n",
        "(0.024570412993081776, 'MMRCurrentRetailCleanPrice')\n",
        "(0.023829947213045752, 'MMRAcquisitonRetailCleanPrice')\n",
        "(0.023319352906372659, 'WarrantyCost')\n",
        "(0.021580108689590424, 'MMRAcquisitionRetailAveragePrice')\n",
        "(0.021386430450262735, 'Trim')\n",
        "(0.019820618615481317, 'VNZIP1')\n",
        "(0.018204244601738764, 'Model')\n",
        "(0.016894578329839677, 'SubModel')\n",
        "(0.016526110453227244, 'Make')\n",
        "(0.016312217165575266, 'Auction')\n",
        "(0.011107426278200171, 'VNST')\n",
        "(0.0070703745204791016, 'Size')\n",
        "(0.006470948528089525, 'Color')\n",
        "(0.0031177854995734049, 'TopThreeAmericanName')\n",
        "(0.0018040243990435482, 'Nationality')\n",
        "(0.00098759338574199167, 'Transmission')\n",
        "(0.00079291042512137774, 'IsOnlineSale')\n",
        "(0.00034305395598610447, 'missing_price')\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testing_X = test[features].values\n",
      "y_pred = RF.predict(testing_X)\n",
      "\n",
      "# Create a submission\n",
      "submission = pd.DataFrame({ 'RefId' : test['RefId'].values, 'prediction' : y_pred })\n",
      "submission.to_csv('submission.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "    Run all above - \n",
      "    Playing around with other classifiers below\n",
      "    \n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "'\\n    Run all above - \\n    Playing around with other classifiers below\\n    \\n'"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "\n",
      "GBC = GradientBoostingClassifier(learning_rate=0.05)\n",
      "GBC.fit(X_split1,y_split1)\n",
      "y_predict1 = GBC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = GBC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.386696730552\n",
        "Split2 F1: 0.372653205809\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.95     29114\n",
        "        1.0       0.84      0.25      0.39      4093\n",
        "\n",
        "avg / total       0.90      0.90      0.88     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.95     15685\n",
        "        1.0       0.84      0.24      0.37      2196\n",
        "\n",
        "avg / total       0.90      0.90      0.88     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier \n",
      "\n",
      "DTC = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
      "DTC.fit(X_split1,y_split1)\n",
      "y_predict1 = DTC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = DTC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.382279385495\n",
        "Split2 F1: 0.371621621622\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.71      0.26      0.38      4093\n",
        "\n",
        "avg / total       0.88      0.90      0.87     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.72      0.25      0.37      2196\n",
        "\n",
        "avg / total       0.88      0.90      0.87     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import AdaBoostClassifier \n",
      "\n",
      "ABC = AdaBoostClassifier()\n",
      "ABC.fit(X_split1,y_split1)\n",
      "y_predict1 = ABC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = ABC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.37386239534\n",
        "Split2 F1: 0.361636301134\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.73      0.25      0.37      4093\n",
        "\n",
        "avg / total       0.88      0.90      0.87     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.74      0.24      0.36      2196\n",
        "\n",
        "avg / total       0.88      0.90      0.87     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "GNB = GaussianNB()\n",
      "GNB.fit(X_split1,y_split1)\n",
      "y_predict1 = GNB.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = GNB.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.372150222306\n",
        "Split2 F1: 0.356251098998\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.92      0.85      0.88     29114\n",
        "        1.0       0.30      0.48      0.37      4093\n",
        "\n",
        "avg / total       0.84      0.80      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.92      0.84      0.88     15685\n",
        "        1.0       0.29      0.46      0.36      2196\n",
        "\n",
        "avg / total       0.84      0.80      0.81     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "\n",
      "ETC = ExtraTreesClassifier(n_estimators = 100, max_depth =8, min_samples_split=15,)\n",
      "ETC.fit(X_split1,y_split1)\n",
      "y_predict1 = ETC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = ETC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.311111111111\n",
        "Split2 F1: 0.280821917808\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      1.00      0.94     29114\n",
        "        1.0       0.90      0.19      0.31      4093\n",
        "\n",
        "avg / total       0.90      0.90      0.87     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      1.00      0.94     15685\n",
        "        1.0       0.85      0.17      0.28      2196\n",
        "\n",
        "avg / total       0.89      0.89      0.86     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "LR = LogisticRegression(penalty='l1')\n",
      "LR.fit(X_split1,y_split1)\n",
      "y_predict1 = LR.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = LR.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.305813953488\n",
        "Split2 F1: 0.291453615778\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.74      0.19      0.31      4093\n",
        "\n",
        "avg / total       0.88      0.89      0.86     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.74      0.18      0.29      2196\n",
        "\n",
        "avg / total       0.88      0.89      0.86     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import BaggingClassifier \n",
      "\n",
      "BCF = BaggingClassifier(max_samples =300)\n",
      "BCF.fit(X_split1,y_split1)\n",
      "y_predict1 = BCF.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = BCF.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.315244487057\n",
        "Split2 F1: 0.296588868941\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.73      0.20      0.32      4093\n",
        "\n",
        "avg / total       0.88      0.89      0.86     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.70      0.19      0.30      2196\n",
        "\n",
        "avg / total       0.87      0.89      0.86     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "SGD = SGDClassifier(penalty='l1')\n",
      "SGD.fit(X_split1,y_split1)\n",
      "y_predict1 = SGD.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = SGD.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.13810741688\n",
        "Split2 F1: 0.135465116279\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      0.94      0.91     29114\n",
        "        1.0       0.20      0.11      0.14      4093\n",
        "\n",
        "avg / total       0.80      0.84      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      0.94      0.91     15685\n",
        "        1.0       0.19      0.11      0.14      2196\n",
        "\n",
        "avg / total       0.80      0.83      0.81     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB \n",
      "\n",
      "MNB = MultinomialNB(alpha = 0.1)\n",
      "MNB.fit(X_split1,y_split1)\n",
      "y_predict1 = MNB.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = MNB.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.264892814823\n",
        "Split2 F1: 0.256794934243\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.91      0.59      0.71     29114\n",
        "        1.0       0.17      0.60      0.26      4093\n",
        "\n",
        "avg / total       0.82      0.59      0.66     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.91      0.57      0.70     15685\n",
        "        1.0       0.16      0.60      0.26      2196\n",
        "\n",
        "avg / total       0.82      0.57      0.65     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "KNN = KNeighborsClassifier(n_neighbors=6)\n",
      "KNN.fit(X_split1,y_split1)\n",
      "y_predict1 = KNN.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = KNN.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.091533180778\n",
        "Split2 F1: 0.0472573839662\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.94     29114\n",
        "        1.0       0.72      0.05      0.09      4093\n",
        "\n",
        "avg / total       0.86      0.88      0.83     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      0.99      0.93     15685\n",
        "        1.0       0.32      0.03      0.05      2196\n",
        "\n",
        "avg / total       0.81      0.87      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "\n",
      "SV = SVC()\n",
      "SV.fit(X_split1,y_split1)\n",
      "y_predict1 = SV.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = SV.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 1.0\n",
        "Split2 F1: 0\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       1.00      1.00      1.00     29114\n",
        "        1.0       1.00      1.00      1.00      4093\n",
        "\n",
        "avg / total       1.00      1.00      1.00     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.00      0.00      0.00      2196\n",
        "\n",
        "avg / total       0.77      0.88      0.82     17881\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/dino/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
        "  'recall', 'true', average, warn_for)\n",
        "/Users/dino/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 25
    }
   ],
   "metadata": {}
  }
 ]
}