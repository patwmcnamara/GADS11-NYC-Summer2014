{
 "metadata": {
  "name": "",
  "signature": "sha256:6fe8a59292f30ef78a11b9577b480cdcc2c1f3b43e9c40d3edf09d0fd4c4ee31"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# If you aren't running this from `lab_submissions/lab08/$FLASTNAME` then copy it over and start again."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Probability\n",
      "\n",
      "We're in search of the perfect soft-boiled egg. The eggs are from different sources, and we don't have a timer. So the brand of the egg and the length of time they boil are random."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Joint Probability\n",
      "\n",
      "In words, what does $P(brand=Acme, time=optimal)$ refer to? P(BrT) The probability that the brand is Acme and the time is Optimal\n",
      "\n",
      "Is it the same as $P(brand=Acme)$ or $P(time=optimal)$? No, more like P(brand=acme) AND P(time=optimal), Joint probability P(A) * P(B)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conditional Probability\n",
      "\n",
      "In words, what does $P(time=optimal | brand=Acme)$ refer to? What is the probability that time is optimal assuming that we already KNOW that the brand is Acme? Conditional probability.\n",
      "\n",
      "Is it the same as $P(brand=Acme | time=optimal)$?  yes/no no.\n",
      "\n",
      "Is it the same as $P(brand=Acme, time=optimal)$?  yes/no no.\n",
      "\n",
      "What is the relationship between the conditional and the joint probabilities? the probability that the brand is acme in the situation that the cooking time is optimal is the same as the probability that the brand is acme and the cooking time is optimal all divided by the probability that the time is optimal. P(A|B)=P(AB)/P(B)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Independence\n",
      "\n",
      "It's not unreasonable to think the optimal boil time has nothing to do with the brand of the egg. In this case, how do the conditional probabilities change? Well, in that case, we can assume that the outcome of the cooking of the egg is not determined by the brand of egg used at all.  P(A|B) = P(A)  The assumption that the probability of B is irrelevant.\n",
      "\n",
      "What is now the expression for the joint probability in terms of the new, simplified joint probabilities? P(A|B) = P(A)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayes's Theorem\n",
      "\n",
      "Let's assume the different egg brands actually do have different ideal boil times. i.e. let's drop the independence assumption.\n",
      "\n",
      "We've boiled 100 eggs now. When we ate one that was cooked ideally, we took the pains to test the brand of the egg and found that of the 10 ideal eggs, 5 were Acme. We also know that we have 5 brands in our basket.\n",
      "\n",
      "Since Acme eggs seemed to perform well, we bought a bunch of Acme eggs. Given that we know we're about to crack open an Acme egg, what is a reasonable estimation of the probability that the egg is boiled ideally? What is $P(time=optimal | brand=Acme)$?\n",
      "\n",
      "Bayes's Theorem:\n",
      "P(A|B) = P(B|A) * P(A) / P(B)\n",
      "\n",
      "Determined from the word problem:\n",
      "P(A) = the probability that the brand is acme = 1 / 5\n",
      "P(B) = the probability that the cooking time is optimal = 1 / 10\n",
      "P(B|A) = 1/2\n",
      "\n",
      "Find P(A|B)\n",
      "\n",
      "P(A|B) = 0.5 * 0.2 * 0.1 = 0.01\n",
      "\n",
      "In English:\n",
      "The probability of the brand being acme given the cook time is optimal equals the probability that the time is optimal given the brand is acme times the probability that the brand is acme all divided by the probability that the cooking time is optimal.\n",
      "\n",
      "So, it would appear that the probability of the brand being acme given the cook time is optimal is one out of a hundred."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Naive Bayes\n",
      "Naive Bayes is where we make the assumption that A and B are independent of each other.  This allows for much faster processing of data sets because multi-level relationships need not be tracked."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Optional: Proof\n",
      "\n",
      "Starting with the equation in the slides, prove that the independence assumption leads directly to the final form. i.e., show that $P(x_1, x_2, ..., x_n | C)$ leads to $P(x_1 | C) \\cdot P(x_2 | C) \\cdot P(x_3 | C) \\cdot \\dots P(x_n | C)$\n",
      "\n",
      "Show work here:\n",
      "\n",
      "...\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multinomial vs. Bernoulli NB:\n",
      "![](http://note.io/1ftU7X6)\n",
      "\n",
      "**Multinomial Model** actually counts occurences out of all possible occurences for probability - better for greater features\n",
      "\n",
      "The class equals:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Beijing|C) \\cdot P(and|C) \\cdot P(Taipei|C) \\cdot P(join|C) \\cdot P(WTO|C)$$\n",
      "over all classes C.\n",
      "\n",
      "**Bernoulli model** counts only all documents with presence of the word - better for fewer features\n",
      "\n",
      "The class equals:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Alaska=0|C) \\cdot P(Beijing=1|C) \\cdot P(India=0|C) \\cdot P(join=1|C) \\cdot P(Taipei=1|C) \\cdot P(WTO=1|C)$$\n",
      "over all classes C."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB  # Naive Bayes model classes\n",
      "from sklearn.feature_extraction.text import CountVectorizer  # convert text into feature vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "### How the Count Vectorizer Works -- Just run and read for the next few cells\n",
      "#\n",
      "\n",
      "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']\n",
      "print \"Original text is:\\n\", '\\n'.join(text)\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "print\n",
      "print 'features'\n",
      "print list(enumerate(vectorizer.get_feature_names()))\n",
      "\n",
      "# call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)\n",
      "print\n",
      "print '(document, feature)  count'\n",
      "print x\n",
      "print\n",
      "print 'the same data in matrix form:'\n",
      "print x.toarray()\n",
      "\n",
      "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
      "# just their frequency"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original text is:\n",
        "Math is great\n",
        "Math is really great\n",
        "Exciting exciting Math\n",
        "\n",
        "features\n",
        "[(0, u'exciting'), (1, u'great'), (2, u'is'), (3, u'math'), (4, u'really')]\n",
        "\n",
        "(document, feature)  count\n",
        "  (0, 1)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 3)\t1\n",
        "  (1, 1)\t1\n",
        "  (1, 2)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 3)\t1\n",
        "\n",
        "the same data in matrix form:\n",
        "[[0 1 1 1 0]\n",
        " [0 1 1 1 1]\n",
        " [2 0 0 1 0]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can include bigrams and larger in your model as well\n",
      "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
      "\n",
      "vectorizer.fit(text)\n",
      "print\n",
      "print 'features'\n",
      "print list(enumerate(vectorizer.get_feature_names()))\n",
      "\n",
      "x = vectorizer.transform(text)\n",
      "print\n",
      "print '(document, feature)  count'\n",
      "print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "features\n",
        "[(0, u'exciting'), (1, u'exciting exciting'), (2, u'exciting math'), (3, u'great'), (4, u'is'), (5, u'is great'), (6, u'is really'), (7, u'math'), (8, u'math is'), (9, u'really'), (10, u'really great')]\n",
        "\n",
        "(document, feature)  count\n",
        "  (0, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (1, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (1, 8)\t1\n",
        "  (1, 9)\t1\n",
        "  (1, 10)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 7)\t1\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wikipedia\n",
      "\n",
      "## Now let's fetch some real text samples.\n",
      "\n",
      "Little known fact: The greatest value of Wikipedia lies not in the incredibly broad collection of human knowledge, but in providing Machine Learning researchers with an easily accessible corpus with which to train their models.\n",
      "\n",
      "Just kidding. Sort of.\n",
      "\n",
      "### In your terminal, type `pip install wikipedia`. If that doesn't work, `sudo pip install wikipedia`.\n",
      "\n",
      "We're going to fetch a bunch of documents from the English and Simple English versions of Wikipedia. We'll train our classifier on these documents and then see if it can guess whether new documents came from one or the other.\n",
      "\n",
      "The fun begins...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import wikipedia as wiki  # wikipedia api wrapper\n",
      "wiki.set_rate_limiting(True)  # might actually speed things up.\n",
      "from sklearn.cross_validation import train_test_split  # split the data you have into training and test sets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fetch_wiki(title, lang):\n",
      "    '''\n",
      "    Return the regular English or simple versions of an article.\n",
      "    Simple versions are far shorter than the regular ones, so only\n",
      "    pull the summary of regular articles.\n",
      "    In case of an error, just return None instead of crashing the program.\n",
      "    '''\n",
      "    assert lang in ('en', 'simple'), \"Language must be 'en' or 'simple'\"\n",
      "\n",
      "    try:\n",
      "        wiki.set_lang(lang)\n",
      "        page = wiki.page(title)\n",
      "        # print page.title  # used for testing the function\n",
      "        return (page.summary, 1) if lang == 'en' else (page.content, 0)  # 1: english, 0: simple\n",
      "    except:  # NOTE: you should never have a blind `except` like this. but, hey, we're hacking.\n",
      "        print ' - error with ' + lang + ' page for: ' + title\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = ['General Relativity', 'Bayes Theorem', 'Ada Lovelace',\n",
      "            'jackfruit', 'mantis shrimp', 'Der Ring des Nibelungen',\n",
      "            'antikythera mechanism', 'teflon', 'superconductor',\n",
      "            'Harper Lee', 'durian', 'Shostakovich', 'steel',\n",
      "            'database', 'transistor', 'Goethe', 'dog', 'meme', 'spleen',\n",
      "            'morphine', 'maple', \"rubik's cube\", 'souffle', 'chlorine',\n",
      "            'earthworm', 'prune', 'ballet', 'ultrasound', 'bruce lee']\n",
      "\n",
      "corpus = []\n",
      "\n",
      "# search for each article, and if we get a result, store it in the corpus\n",
      "for article in articles:\n",
      "    en = fetch_wiki(article, 'en')\n",
      "    if en:\n",
      "        corpus.append(en)\n",
      "    simple = fetch_wiki(article, 'simple')\n",
      "    if simple:\n",
      "        corpus.append(simple)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "text, Y = zip(*corpus)\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 20.00%\n",
        "Accuracy: 60.00%\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# More data\n",
      "\n",
      "That didn't work so well. You can repeat the last cell a few times to see much the accuracy depends on the train/test split. This calls for more data.\n",
      "\n",
      "Use the `.random(n_titles)` method to retrieve a list of 100 more titles and fetch the text for training. Note that `.random` will only return up to 10 titles at a time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki.set_lang('simple')\n",
      "\n",
      "for i in xrange(10):\n",
      "    print i  # monitor progress -- this is really slow\n",
      "    for title in wiki.random(10):\n",
      "        # your turn! Find each title in english and simple in the wiki and if you get a result, store it to corpus.\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-9-865a437c4111>, line 6)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-865a437c4111>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    # your turn! Find each title in each language in wiki and if you get a result, store it to corpus.\u001b[0m\n\u001b[0m                                                                                                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we just spent a long time fetching this data, so let's save it so we don't have to do it all over again.\n",
      "import pickle\n",
      "pickle.dump(corpus, open('corpus.p', 'wb'))\n",
      "\n",
      "# restore with: corpus = pickle.load(open('corpus.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "text, Y = zip(*corpus)\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variance is a bit contained, but it's still not performing too well. Now it's your turn to try a few things to see if you can improve the performance to something useful. You can try fetching even more data overnight. We'd only fetched the English article summaries, but maybe it'll help to use the full articles. You can try filtering out words < 3 or 4 chars long. We haven't tried allowing bigrams and trigrams yet or removed stop-words. Both the Multinomial and Bernoulli models have model parameters that you can try adjusting. Finally, you might imagine other ways to tweak our model!\n",
      "\n",
      "Here are links to relevant docs:\n",
      "\n",
      "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
      "[MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
      "[BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# start here!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}