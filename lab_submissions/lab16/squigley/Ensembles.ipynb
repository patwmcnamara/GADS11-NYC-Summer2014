{
 "metadata": {
  "name": "",
  "signature": "sha256:81c35e4e7e50a9013d2adebc898c0ae28628613790789e6c6c1588fd8ee3abe7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Ploting decision surfaces of ensembles of trees on the iris dataset\n",
      "\n",
      "Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset.\n",
      "\n",
      "This plot compares the decision surfaces learned by a decision tree classifier (first column), by a random forest classifier (second column), by an extra-trees classifier (third column) and by an AdaBoost classifier (fourth column).\n",
      "\n",
      "In the first row, the classifiers are built using the sepal width and the sepal length features only, on the second row using the petal length and sepal length only, and on the third row using the petal width and the petal length only."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pylab as pl\n",
      "\n",
      "from sklearn import clone\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
      "from sklearn.ensemble.forest import RandomForestClassifier, ExtraTreesClassifier\n",
      "\n",
      "from sklearn.externals.six.moves import xrange\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Parameters\n",
      "n_classes = 3\n",
      "n_estimators = 30\n",
      "plot_colors = \"bry\"\n",
      "plot_step = 0.02\n",
      "\n",
      "# Load data\n",
      "iris = load_iris()\n",
      "\n",
      "plot_idx = 1\n",
      "\n",
      "for pair in ([0, 1], [0, 2], [2, 3]):\n",
      "    for model in (DecisionTreeClassifier(),\n",
      "                  RandomForestClassifier(n_estimators=n_estimators),\n",
      "                  ExtraTreesClassifier(n_estimators=n_estimators),\n",
      "                  AdaBoostClassifier(DecisionTreeClassifier(),\n",
      "                                     n_estimators=n_estimators)):\n",
      "        # We only take the two corresponding features\n",
      "        X = iris.data[:, pair]\n",
      "        y = iris.target\n",
      "\n",
      "        # Shuffle\n",
      "        idx = np.arange(X.shape[0])\n",
      "        np.random.seed(13)\n",
      "        np.random.shuffle(idx)\n",
      "        X = X[idx]\n",
      "        y = y[idx]\n",
      "\n",
      "        # Standardize\n",
      "        mean = X.mean(axis=0)\n",
      "        std = X.std(axis=0)\n",
      "        X = (X - mean) / std\n",
      "\n",
      "        # Train\n",
      "        clf = model.fit(X, y)\n",
      "\n",
      "        # Get accuracy scores\n",
      "        scores = clf.score(X, y)\n",
      "        # Create a title for each column and the console by using str() and slicing away useless parts of the string\n",
      "        model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]\n",
      "        model_details = model_title\n",
      "        if hasattr(model, \"estimators_\"):\n",
      "            model_details += \" with {} estimators\".format(len(model.estimators_))\n",
      "        print model_details + \" with features\", pair, \"has a score of\", scores\n",
      "\n",
      "        # Plot the decision boundary\n",
      "        pl.subplot(3, 4, plot_idx)\n",
      "\n",
      "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
      "                             np.arange(y_min, y_max, plot_step))\n",
      "\n",
      "        if isinstance(model, DecisionTreeClassifier):\n",
      "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "            Z = Z.reshape(xx.shape)\n",
      "            cs = pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "        else:\n",
      "            for tree in model.estimators_:\n",
      "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "                Z = Z.reshape(xx.shape)\n",
      "                cs = pl.contourf(xx, yy, Z, alpha=0.1, cmap=pl.cm.Paired)\n",
      "\n",
      "        pl.axis(\"tight\")\n",
      "\n",
      "        # Plot the training points\n",
      "        for i, c in zip(xrange(n_classes), plot_colors):\n",
      "            idx = np.where(y == i)\n",
      "            pl.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],\n",
      "                       cmap=pl.cm.Paired)\n",
      "\n",
      "        pl.axis(\"tight\")\n",
      "\n",
      "        plot_idx += 1\n",
      "\n",
      "pl.suptitle(\"Decision surfaces of DecisionTreeClassifier, \"\n",
      "            \"RandomForestClassifier,\\nExtraTreesClassifier\"\n",
      "            \" and AdaBoostClassifier\")\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTree with features [0, 1] has a score of 0.926666666667\n",
        "RandomForest with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 1] has a score of 0.926666666667\n",
        "ExtraTrees with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 1] has a score of 0.926666666667\n",
        "AdaBoost with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 1] has a score of 0.926666666667\n",
        "DecisionTree with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "RandomForest with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "ExtraTrees with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "AdaBoost with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [0, 2] has a score of 0.993333333333\n",
        "DecisionTree with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [2, 3] has a score of 0.993333333333\n",
        "RandomForest with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [2, 3] has a score of 0.993333333333\n",
        "ExtraTrees with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [2, 3] has a score of 0.993333333333\n",
        "AdaBoost with 30 estimators with features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [2, 3] has a score of 0.993333333333\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Application to other data sets\n",
      "\n",
      "Here we have a dataset of chapters from books and plays by specific authors, and their usages of stop words. Let's see how accurately a random forest can predict the author based on stop word usage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Importing libraries\n",
      "import random\n",
      "from pandas import read_csv\n",
      "from sklearn.cross_validation import train_test_split, Bootstrap\n",
      "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "from sklearn import preprocessing\n",
      "\n",
      "# Read in the authorship csv file.\n",
      "authorship = read_csv('http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv')\n",
      "\n",
      "# Setting a list to represent all the authors in the file, and then using LabelEncoder to set them to ints.\n",
      "authors = list(set(authorship.Author.values))\n",
      "le = preprocessing.LabelEncoder()\n",
      "le.fit(authors)\n",
      "authorship['Author_num'] = le.transform(authorship['Author']) # Actually sets the author's \"number\" or id.\n",
      "\n",
      "#What are some of the stop words we're looking at?\n",
      "features = list(authorship.columns)\n",
      "features\n",
      "features.remove('Author')\n",
      "features.remove('Author_num')\n",
      "\n",
      "# Create a random variable (random forests work best with a random variable)\n",
      "# and create a test and training set\n",
      "authorship['random'] = [random.random() for i in range(841)]\n",
      "x_train, x_test, y_train, y_test = train_test_split(authorship[features],\n",
      "                                                    authorship.Author_num.values,\n",
      "                                                    test_size=0.4,\n",
      "                                                    random_state=123)\n",
      "\n",
      "## Compare the results here of random forests, adaboost, and a non-ensembled naive bayes classifier.\n",
      "clf = ExtraTreesClassifier(n_estimators=1, random_state=10).fit(x_train, y_train)\n",
      "print ExtraTreesClassifier.__name__, 'results'\n",
      "print metrics.confusion_matrix(clf.predict(x_test), y_test)\n",
      "\n",
      "clf = AdaBoostClassifier(n_estimators=1, random_state=10).fit(x_train, y_train)\n",
      "print AdaBoostClassifier.__name__, 'results'\n",
      "print metrics.confusion_matrix(clf.predict(x_test), y_test)\n",
      "\n",
      "clf = MultinomialNB().fit(x_train, y_train)\n",
      "print MultinomialNB.__name__, 'results'\n",
      "print metrics.confusion_matrix(clf.predict(x_test), y_test)\n",
      "## Sometimes a simpler answer is a better approach!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ExtraTreesClassifier results\n",
        "[[119   7   0   9]\n",
        " [  9  81   4  14]\n",
        " [  0   7  16   4]\n",
        " [  8   4   0  55]]\n",
        "AdaBoostClassifier results\n",
        "[[131  14   4  62]\n",
        " [  5  85  16  20]\n",
        " [  0   0   0   0]\n",
        " [  0   0   0   0]]\n",
        "MultinomialNB results\n",
        "[[136   2   0   0]\n",
        " [  0  93   0   0]\n",
        " [  0   0  20   0]\n",
        " [  0   4   0  82]]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What if we brute force our way to the right answer? (Slow computers, be cautious!)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = ExtraTreesClassifier(n_estimators=10000, random_state=10).fit(x_train, y_train)\n",
      "print ExtraTreesClassifier.__name__, 'results'\n",
      "print metrics.confusion_matrix(clf.predict(x_test), y_test)\n",
      "\n",
      "clf = AdaBoostClassifier(n_estimators=10000, random_state=10).fit(x_train, y_train)\n",
      "print AdaBoostClassifier.__name__, 'results'\n",
      "print metrics.confusion_matrix(clf.predict(x_test), y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ExtraTreesClassifier results\n",
        "[[135   1   0   0]\n",
        " [  1  98   0   1]\n",
        " [  0   0  20   0]\n",
        " [  0   0   0  81]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "AdaBoostClassifier"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " results\n",
        "[[133   0   0   0]\n",
        " [  3  98   0   0]\n",
        " [  0   0  18   0]\n",
        " [  0   1   2  82]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Timing\n",
      "\n",
      "Try timing the above on your machine (you can use datetimes to get a simple approach here)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime\n",
      "\n",
      "a = datetime.now()\n",
      "clf = ExtraTreesClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\n",
      "print \"High Number of Trees Timing:\", datetime.now() - a\n",
      "    \n",
      "a = datetime.now()\n",
      "clf = AdaBoostClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\n",
      "print \"AdaBoost Timing:            \", datetime.now() - a\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " High Number of Trees Timing: 0:00:00.099675\n",
        "AdaBoost Timing:            "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0:00:00.326324\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}