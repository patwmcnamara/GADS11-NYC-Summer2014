{
 "metadata": {
  "name": "",
  "signature": "sha256:807536211062ee6e7d8f0a4de89714dbf650871e581f6843d5693552ae2dc494"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# If you aren't running this from `lab_submissions/lab08/$FLASTNAME` then copy it over and start over."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 0: Loading all the libraries for lab today"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# note which libraries these classes and functions come from\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.feature_selection import f_regression\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 1: Linear Algebra Practice"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell\n",
      "a = np.random.randint(0, 8, 20).reshape(4, 5)\n",
      "b = np.random.randint(0, 8, 15).reshape(3, 5)\n",
      "print a, a.shape\n",
      "print b, b.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[4 6 2 0 5]\n",
        " [5 0 4 0 6]\n",
        " [2 7 5 7 7]\n",
        " [6 7 2 5 1]] (4, 5)\n",
        "[[6 7 6 1 0]\n",
        " [5 1 3 3 5]\n",
        " [1 0 2 2 2]] (3, 5)\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can you multiply these together?\n",
      "\n",
      "If not, how could you manipulate these so they can be multiplied?\n",
      "\n",
      "What shape would the output have?\n",
      "\n",
      "Try multiplying these using numpy and confirm your guess."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# code\n",
      "#a=np.asmatrix(a)\n",
      "#b=np.asmatrix(b)\n",
      "#b=b.reshape(5,3)\n",
      "\n",
      "print np.dot(a,b.T)\n",
      "c = np.dot(a,b.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 78  57  18]\n",
        " [ 54  67  25]\n",
        " [ 98  88  40]\n",
        " [102  63  22]]\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now take the output of this multiplication and find its inverse.\n",
      "\n",
      "J/K, it has to be square. Multiply this matrix by its transpose to get a square matrix, and then find the inverse of that. (There's an ambiguity here, too, in the order. We don't care which order you choose.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# code here\n",
      "print np.dot(c,c.T)\n",
      "print np.dot(c,c.T) ** -1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 9657  8481 13380 11943]\n",
        " [ 8481  8030 12188 10279]\n",
        " [13380 12188 18948 16420]\n",
        " [11943 10279 16420 14857]]\n",
        "[[ -1.90872788e+12   9.55691288e+11  -5.57486585e+11   1.48928559e+12]\n",
        " [  9.55691288e+11  -4.78510242e+11   2.79130974e+11  -7.45678460e+11]\n",
        " [ -5.57486585e+11   2.79130974e+11  -1.62826402e+11   4.34979102e+11]\n",
        " [  1.48928559e+12  -7.45678460e+11   4.34979102e+11  -1.16201560e+12]]\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 2: Least Squares Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](http://note.io/1fqixjZ)\n",
      "[source](http://sydney.edu.au/stuserv/documents/maths_learning_centre/matrixmodule8.pdf)\n",
      "\n",
      "Penn State has this [great source](https://onlinecourses.science.psu.edu/stat501/node/59) for easy to understand background on the matrix algebra behind regression.\n",
      "\n",
      "* *Matrix Addition*: Two matrices can be added together only if they have the same number of rows and columns. \n",
      "* _Matrix Multiplication:_ Two matrices can be multiplied together only if the number of columns of the first matrix equals the number of rows of the second matrix. \n",
      "\n",
      "\n",
      "The point of all this is to get that Betas vector:\n",
      "![](http://note.io/1mSiPQl)\n",
      "\n",
      "Those \"1s\" in the first column are just there so we have a value for the intercept.  Without a scalar intercept, we can skip them.\n",
      "\n",
      "####Matrix Multiplication Details:\n",
      "![](http://note.io/1mSlfyz)\n",
      "\n",
      "A good cheatsheet a student made for a final: [link](http://www.scribd.com/doc/46495128/FinalCheetSheet)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Optional: Use numpy to write the linear regression formula\n",
      "Keep in mind that we know x and y, and can assume that \u03b1 (intercept) is 1 in order to solve for the coefficients \u03b2\n",
      "\n",
      "Linear Regression Formula\n",
      "$$y = \\alpha + \\beta x + \\epsilon$$\n",
      "\n",
      "Linear Regression solved for coefficients \n",
      "$$\\beta = \\left({X}^\\top {X}\\right) ^{-1} {X}^\\top y$$\n",
      "\n",
      "In a moment, we'll use [sklearn's implementation](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/base.py#L331). If you're so inclined, try implementing this yourself."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ols_regression(x_input, y_response):\n",
      "    \"\"\"solution for solving a regression with ordinary least squares.\n",
      "    x_input: an input narray (X)\n",
      "    y_response: a 1d array of expected outputs (y)\n",
      "    X and y must have equal lengths, but x can be multiple dimensions\n",
      "    should return back a 1d array of 1 intercept + all coefficients\n",
      "    \"\"\"\n",
      "    # fill in code here.\n",
      "    forcoefs=np.dot(np.dot(x_input.T,x_input) ** -1,np.dot(x_input.T,y_response))\n",
      "    lowerx = \n",
      "    intercept=y_response - (forcoefs * lowerx) - 0\n",
      "    return intercept,forcoefs\n",
      "\n",
      "\n",
      "Xvar = np.array([3.385, 0.48, 1.35, 465, 36.33])\n",
      "yvar = np.array([44.5, 15.5, 8.1, 423, 119.5])\n",
      "beta = ols_regression(Xvar, yvar)\n",
      "print beta  # should return: [37.2008960793 0.838218755695]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(array([ 41.36943604,  15.05607956,   6.85147375,  -7.04792898,  85.90077148]), 0.92483425587595913)\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, write a function that would use this output to predict new data elements (finish the return statement). Are the predicted values close to the true values for the training inputs?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ols_predict(x_input, beta):\n",
      "    \"\"\"solution for predicting new data.\n",
      "    x_input: an input narray (X)\n",
      "    y_intercept: a float\n",
      "    coefficients: matrix of coefficients\n",
      "    X must be as wide as the length of beta.\n",
      "    should return back a 1d array of predicted y-values\n",
      "    \"\"\"\n",
      "    ## PUT CODE HERE\n",
      "    return \n",
      "\n",
      "ols_predict(Xvar, beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Part 2: Working with sklearn and data\n",
      "Here we'll work with a very simple data set of one input (animal body weight) to find the relationship with a response (animal brain weight)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell\n",
      "url = 'https://gist.githubusercontent.com/podopie/5ea0c35ecc556d6cbae3/raw/c56f694bf4e7bbeeec92e24d33a8f49f7da37be8/mammals.csv'\n",
      "animals = pd.read_csv(url)\n",
      "print animals.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              body        brain\n",
        "count    62.000000    62.000000\n",
        "mean    198.789984   283.134194\n",
        "std     899.158011   930.278942\n",
        "min       0.005000     0.140000\n",
        "25%       0.600000     4.250000\n",
        "50%       3.342500    17.250000\n",
        "75%      48.202500   166.000000\n",
        "max    6654.000000  5712.000000\n",
        "\n",
        "[8 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make some plots to explore the data. I'll get you started\n",
      "\n",
      "fig = plt.figure(figsize=(15, 5))\n",
      "\n",
      "# histogram the body weights\n",
      "ax1 = fig.add_subplot(1, 3, 1)  # one row, three columns, first plot\n",
      "ax1.hist(animals['body'])\n",
      "\n",
      "# histogram the brain weights\n",
      "ax1 = fig.add_subplot(1, 3, 2)  # one row, three columns, first plot\n",
      "# fill in code\n",
      "\n",
      "# scatterplot body weight vs. brain weight\n",
      "\n",
      "# optional: Come back to this later and add some styling to the plots. titles, labels, colors, binsizes, etc.\n",
      "\n",
      "fig.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/lib/python2.7/site-packages/matplotlib/figure.py:371: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
        "  \"matplotlib is currently using a non-GUI backend, \"\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAE4CAYAAAC39pw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK0DJxGzlHfsAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x109085510>"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at the numerical data here, it's clear we don't have the best data for a linear reqression, which works best with data that are normally distributed.\n",
      "\n",
      "However, we can take advantage of a power law to \"refit\" our data. We can use a log-log relationship because:\n",
      "\n",
      "* All data are positive, and you can't take the log of 0 or negative numbers.\n",
      "* We have a very heavy long tail distribution, so log-log power law might be a good refitting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](http://note.io/1mSr9jd)\n",
      "\n",
      "_Log transformations make positively skewed distributions more normal (around the mean)_\n",
      "\n",
      "_Warning: lots of distributions look like power laws when there isn't much data_"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# copy the data exploration plots you had in the previous code cell, but take the log of the data.\n",
      "\n",
      "fig = plt.figure(figsize=(15, 5))\n",
      "\n",
      "# histogram the log body weights\n",
      "ax1 = fig.add_subplot(1, 3, 1)  # one row, three columns, first plot\n",
      "ax1.hist(np.log(animals['body']))\n",
      "\n",
      "# histogram the log brain weights\n",
      "ax1 = fig.add_subplot(1, 3, 2)  # one row, three columns, first plot\n",
      "# fill in code\n",
      "\n",
      "# scatterplot log body weight vs. log brain weight\n",
      "\n",
      "# optional: Come back to this later and add some styling to the plots. titles, labels, colors, binsizes, etc.\n",
      "\n",
      "fig.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using sklearn, we will implement solutions to both the original feature space as well as the log transformation. You can also check your own implementation from above if you did that portion. [docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# linear fit\n",
      "# instantiate class\n",
      "linear_fit = LinearRegression()\n",
      "# train/fit model to predict brain weights given the body weights\n",
      "linear_fit.fit(animals[['body']].values, animals['brain'].values)\n",
      "# print beta parameters (should be all positive)\n",
      "print \"sklearn intercept and coef (linear):\", linear_fit.intercept_, linear_fit.coef_\n",
      "\n",
      "# log fit\n",
      "# instantiate class\n",
      "log_fit = LinearRegression()\n",
      "\n",
      "# code here\n",
      "# train/fit model\n",
      "# print beta parameters\n",
      "\n",
      "\n",
      "# uncomment if you did the optional portion above\n",
      "# print \"My intercept and coefficient:\", ols_regression(animals['body'], animals['brain'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot model against data\n",
      "\n",
      "rawbrains = animals['brain'].values\n",
      "rawbods = animals[['body']].values\n",
      "\n",
      "# what does the model predict the body weights to be for the brain weights in rawbods\n",
      "predictrawbrains = # Code here\n",
      "\n",
      "# scatter plot the raw data\n",
      "plt.scatter(rawbods,rawbrains)\n",
      "# plot the predicted brain weights against the raw body weights\n",
      "plt.plot(rawbods,predictrawbrains)\n",
      "# labels\n",
      "plt.ylabel('Brain Size')\n",
      "plt.xlabel('Body Weight')\n",
      "plt.title('Body Weight vs. Brains')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat the previous cell with the log data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Part 3: Scoring the performance of a regression\n",
      "sklearn has built in functionality to score the performance of our regression, but there are multiple ways to dig further into the performance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####R-squared (Score)\n",
      "R squared is a value of performance between 0 and 1. You can think 1 is a \"perfect fit.\" It's the proportion of the variance in the data that is accounted by the model.\n",
      "\n",
      "Extra reading: <a href=\"http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit\">Understanding R-Sqared and Goodness of fit</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Linear R-squared\", round(linear_fit.score( # fill in the right args # ), 4)\n",
      "print \"Log R-squared\", round(log_fit.score( # fill in the right args # ), 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### P-values of individual features\n",
      "\n",
      "p-values provide an understanding of *significance* to a feature. In best practices, we'd usually find p-values of features first to determine what features are best to use in a regression.\n",
      "\n",
      "sklearn's [f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) returns back values from an F-Test and the feature p-values, where the F-Test tests for variance, and p-values shows the probability of an event occuring to change. In this case, a low p-value (and high F-statistic) is best. In bio sciences, they generally only accept `p < 0.05`, but for your own work, you will have to determine what is best for you."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# F scores of raw data\n",
      "lin_f, lin_p = f_regression( # fill in the right args # )\n",
      "# F scores of log data\n",
      "log_f, log_p = f_regression( # fill in the right args # )\n",
      "print 'LINEAR F-Test Values:', lin_f[0]\n",
      "print 'LINEAR p-values:     ', lin_p[0]\n",
      "print 'LOG    F-Test Values:', log_f[0]\n",
      "print 'LOG    p-values:     ', log_p[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Mean Squared Error (and mean absolute error)\n",
      "\n",
      "Both mean squared error (MSE) and mean absolute error (MAE) help us understand the average error by distance between our predicted values and our actual values. While both are relatively simple to calculate, sklearn also includes functions for this.\n",
      "\n",
      "While MAE provides a general context in error, MSE can help evaluate strength of outliers in the model.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell\n",
      "linear_prediction = linear_fit.predict(animals[['body']].values)\n",
      "log_prediction = log_fit.predict(np.log(animals[['body']].values))\n",
      "\n",
      "# pay attention to the arguments and be sure you understand why they are correct\n",
      "print 'LINEAR  MSE:', mean_squared_error(linear_prediction, animals[['body']].values)\n",
      "print 'LINEAR RMSE:', np.sqrt(mean_squared_error(linear_prediction, animals[['body']].values))\n",
      "print 'LINEAR  MAE:', mean_absolute_error(linear_prediction, animals[['body']].values)\n",
      "print\n",
      "print 'LOG     MSE:', mean_squared_error(log_prediction, np.log(animals[['body']].values))\n",
      "print 'LOG    RMSE:', np.sqrt(mean_squared_error(log_prediction, np.log(animals[['body']].values)))\n",
      "print 'LOG     MAE:', mean_absolute_error(log_prediction), np.log(animals[['body']].values))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Statsmodels\n",
      "In some efforts, statsmodels is another tool in python that provides some similar functionality to sklearn. We will occasionally use it in the future, but wanted to introduce it here as an example of work that may be more familiar to R users in class:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell\n",
      "import statsmodels.formula.api as sm\n",
      "fit1 = sm.ols(formula='brain ~ body', data=animals).fit()\n",
      "print fit1.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this cell\n",
      "fit2 = sm.ols(formula='np.log(brain) ~ np.log(body)', data=animals).fit()\n",
      "print fit2.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Next steps: Lab submission"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you're ambitious and want more practice doing linear regression with sklearn, continue on. Otherwise, please submit this file as a pull request."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Optional: Models with Bike Share"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In folder `/data` you will find bike share data (it is also included in lesson05), with a text file that explains the features included.\n",
      "For a lab submission, include either a python script or an ipython notebook that shows you going through the following steps:\n",
      "\n",
      "1. Go through the ACES model (assemble, clean, explore, subset) for data exploration and analysis\n",
      "2. Evaluate features that contribute to the solution of predicting casual, registered, and cnt\n",
      "3. Build three predictive models that use the numeric data available in order to predict each.\n",
      "\n",
      "If you want to work with non numerical data, consider looking into the pandas feature for <a href='http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.core.reshape.get_dummies.html'>getting dummy features</a>.\n",
      "\n",
      "Try starting with the `day.csv` file first. Those looking for a challenge, try working on the `hour.csv` file, but first start with the models for `day.csv`.\n",
      "\n",
      "Please submit as much of this as you feel like attempting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Casual = count of casual users\n",
      "# Registered: Count of registerd users\n",
      "# CNT: Combined count, aka total\n",
      "\n",
      "# Key Features\n",
      "## Holiday\n",
      "## Weekday\n",
      "## Working Day\n",
      "## Weather Sit\n",
      "## ATemp (feeling temp)\n",
      "## Windspeed\n",
      "## Humidity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}